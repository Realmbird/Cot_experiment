{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28fe00ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of base_solution.json:\n",
      "{\n",
      "  \"prompt\": \"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{}. Problem: Compute\\n\\n$3(1+3(1+3(1+3(1+3(1+3(1+3(1+3(1+3(1+3)))))))))$ Solution: \\n<think>\\n\",\n",
      "  \"solution\": \"Alright, so I've got this math problem here that I need to solve. It's a nested expression with a bunch of threes and parentheses. Let me write it down to get a better look:\\n\\n3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3)))))))))\\n\\nWow, that's a lot of parentheses! Okay, so I need to compute this step by step. I think the best way to approach it is to start from the innermost parentheses and work my way outwards. That usually makes nested expressions easier to handle. Let me try that.\\n\\nFirst, let's identify the innermost part. It seems like the last set of parentheses is (1 + 3). Let me compute that first.\\n\\n1 + 3 = 4\\n\\nOkay, so the innermost part simplifies to 4. Now, replacing that in the expression, the next layer out would be 3(1 + 3(1 + 3(...))). Let me replace the innermost parentheses and see what the next step is.\\n\\nSo after replacing, the expression becomes:\\n\\n3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(4))))))))) )\\n\\nWait, hold on, I think I might have miscounted the layers. Let me go step by step.\\n\\nStarting from the innermost:\\n\\n1. Compute 1 + 3 = 4\\n2. The next layer is 1 + 3(4). Let's compute that.\\n\\nSo, 1 + 3(4) = 1 + 12 = 13\\n\\nWait, that seems too big. Maybe I'm not structuring this correctly. Let me try a different approach. Maybe it's better to evaluate each layer one by one, starting from the innermost.\\n\\nLet me number the layers from the innermost outwards. Let's say the innermost is Layer 1, then Layer 2, and so on, until Layer 9.\\n\\nLayer 1: 1 + 3 = 4\\n\\nLayer 2: 1 + 3(Layer 1) = 1 + 3(4) = 1 + 12 = 13\\n\\nLayer 3: 1 + 3(Layer 2) = 1 + 3(13) = 1 + 39 = 40\\n\\nLayer 4: 1 + 3(Layer 3) = 1 + 3(40) = 1 + 120 = 121\\n\\nLayer 5: 1 + 3(Layer 4) = 1 + 3(121) = 1 + 363 = 364\\n\\nLayer 6: 1 + 3(Layer 5) = 1 + 3(364) = 1 + 1092 = 1093\\n\\nLayer 7: 1 + 3(Layer 6) = 1 + 3(1093) = 1 + 3279 = 3280\\n\\nLayer 8: 1 + 3(Layer 7) = 1 + 3(3280) = 1 + 9840 = 9841\\n\\nLayer 9: 1 + 3(Layer 8) = 1 + 3(9841) = 1 + 29523 = 29524\\n\\nWait, but the original expression starts with a 3 multiplied by all of that. So, the entire expression is 3 times the result of Layer 9.\\n\\nSo, 3 * 29524 = ?\\n\\nLet me compute that.\\n\\n29524 * 3:\\n\\nFirst, 20000 * 3 = 60000\\n\\nThen, 9000 * 3 = 27000\\n\\n500 * 3 = 1500\\n\\n24 * 3 = 72\\n\\nAdding those together:\\n\\n60000 + 27000 = 87000\\n\\n87000 + 1500 = 88500\\n\\n88500 + 72 = 88572\\n\\nSo, the total is 88,572.\\n\\nWait, but let me double-check my calculations because that seems quite large. Maybe I made a mistake in one of the layers.\\n\\nLet me go back and verify each layer step by step.\\n\\nLayer 1: 1 + 3 = 4 \\u2714\\ufe0f\\n\\nLayer 2: 1 + 3*4 = 1 + 12 = 13 \\u2714\\ufe0f\\n\\nLayer 3: 1 + 3*13 = 1 + 39 = 40 \\u2714\\ufe0f\\n\\nLayer 4: 1 + 3*40 = 1 + 120 = 121 \\u2714\\ufe0f\\n\\nLayer 5: 1 + 3*121 = 1 + 363 = 364 \\u2714\\ufe0f\\n\\nLayer 6: 1 + 3*364 = 1 + 1092 = 1093 \\u2714\\ufe0f\\n\\nLayer 7: 1 + 3*1093 = 1 + 3279 = 3280 \\u2714\\ufe0f\\n\\nLayer 8: 1 + 3*3280 = 1 + 9840 = 9841 \\u2714\\ufe0f\\n\\nLayer 9: 1 + 3*9841 = 1 + 29523 = 29524 \\u2714\\ufe0f\\n\\nThen, the entire expression is 3*29524 = 88,572 \\u2714\\ufe0f\\n\\nHmm, that seems consistent. Maybe I didn't make a mistake. Let me think if there's another way to approach this problem.\\n\\nAlternatively, maybe I can recognize a pattern or formula that applies here. Let me see.\\n\\nLooking at the expression, each layer is of the form 1 + 3*(previous layer). So, starting from the innermost, each layer is 1 + 3 times the previous. This is a recursive sequence where each term is defined as a(n) = 1 + 3*a(n-1), with a(1) = 4.\\n\\nWait, actually, in my earlier layers, a(1) was 4, which is 1 + 3. Then a(2) was 13, which is 1 + 3*4, and so on.\\n\\nSo, this is a linear recurrence relation. Maybe I can find a closed-form formula for a(n) and then compute a(9), then multiply by 3.\\n\\nThe recurrence is a(n) = 1 + 3*a(n-1), with a(1) = 4.\\n\\nThis is a nonhomogeneous linear recurrence relation. The general solution can be found by solving the homogeneous part and finding a particular solution.\\n\\nThe homogeneous recurrence is a(n) = 3*a(n-1), which has the solution a(n) = C*3^n.\\n\\nFor the particular solution, since the nonhomogeneous term is constant (1), we can assume a constant particular solution, say a_p.\\n\\nPlugging into the recurrence: a_p = 1 + 3*a_p\\n\\nWait, that gives a_p = 1 + 3*a_p, which simplifies to 0 = 1, which is impossible. So, that suggests that our assumption of a constant particular solution is invalid. Maybe we need to try a linear particular solution, like a_p = k*n.\\n\\nLet me try that. Let a_p = k*n.\\n\\nThen, plugging into the recurrence:\\n\\nk*n = 1 + 3*k*(n-1)\\n\\nSimplify:\\n\\nk*n = 1 + 3k*n - 3k\\n\\nBring all terms to one side:\\n\\nk*n - 3k*n + 3k - 1 = 0\\n\\n-2k*n + 3k - 1 = 0\\n\\nFactor out k:\\n\\nk*(-2n + 3) - 1 = 0\\n\\nThis equation needs to hold for all n, which implies that the coefficients of like terms must be equal on both sides. But since it's a function of n, the coefficients of n and the constants must separately match.\\n\\nLooking at the equation:\\n\\n-2k*n + 3k - 1 = 0\\n\\nThis can be rewritten as:\\n\\n(-2k)n + (3k - 1) = 0\\n\\nFor this to be true for all n, the coefficient of n must be zero, and the constant term must also be zero.\\n\\nSo,\\n\\n-2k = 0  \\u21d2  k = 0\\n\\nAnd,\\n\\n3k - 1 = 0  \\u21d2  3*0 - 1 = -1 \\u2260 0\\n\\nThis is a contradiction, meaning our assumption of a linear particular solution is also invalid. Maybe we need to try a quadratic particular solution, a_p = k*n^2.\\n\\nLet's try that.\\n\\na_p = k*n^2\\n\\nPlugging into the recurrence:\\n\\nk*n^2 = 1 + 3*k*(n-1)^2\\n\\nExpand the right-hand side:\\n\\nk*n^2 = 1 + 3k*(n^2 - 2n + 1)\\n\\nk*n^2 = 1 + 3k*n^2 - 6k*n + 3k\\n\\nBring all terms to the left-hand side:\\n\\nk*n^2 - 3k*n^2 + 6k*n - 3k - 1 = 0\\n\\n(-2k)n^2 + 6k*n - 3k - 1 = 0\\n\\nFor this to hold for all n, the coefficients of each power of n must be zero.\\n\\nSo,\\n\\n-2k = 0  \\u21d2  k = 0\\n\\n6k = 0  \\u21d2  k = 0\\n\\n-3k - 1 = 0  \\u21d2  -0 -1 = -1 \\u2260 0\\n\\nAgain, a contradiction. So, a quadratic particular solution also doesn't work. Maybe I need to try a particular solution of the form a_p = k*n + m, but since that didn't work, maybe I need a different approach.\\n\\nAlternatively, since the recurrence is a(n) = 1 + 3*a(n-1), with a(1) = 4, maybe I can compute the terms iteratively, which I already did earlier, and it gave me a(9) = 29524, leading to 3*29524 = 88,572.\\n\\nGiven that the iterative method is straightforward and I've double-checked each layer, perhaps it's correct. Alternatively, maybe I can look for a pattern in the terms.\\n\\nLet me list out the layers again:\\n\\na(1) = 4\\n\\na(2) = 13\\n\\na(3) = 40\\n\\na(4) = 121\\n\\na(5) = 364\\n\\na(6) = 1093\\n\\na(7) = 3280\\n\\na(8) = 9841\\n\\na(9) = 29524\\n\\nLooking at these numbers: 4, 13, 40, 121, 364, 1093, 3280, 9841, 29524.\\n\\nI notice that each term is approximately 3 times the previous term. For example:\\n\\n13 / 4 = 3.25\\n\\n40 / 13 \\u2248 3.077\\n\\n121 / 40 = 3.025\\n\\n364 / 121 \\u2248 3.008\\n\\n1093 / 364 \\u2248 3.0027\\n\\n3280 / 1093 \\u2248 3.0009\\n\\n9841 / 3280 \\u2248 3.0003\\n\\n29524 / 9841 \\u2248 3.0001\\n\\nSo, as n increases, the ratio approaches 3. That makes sense because the recurrence is a(n) = 1 + 3*a(n-1), so for large n, the 1 becomes negligible, and a(n) \\u2248 3*a(n-1).\\n\\nTherefore, a(n) grows roughly like 3^(n-1). But since we have that constant 1 added each time, it's slightly less than 3^(n-1). However, for n=9, the difference is minimal, as we saw the ratio is already very close to 3.\\n\\nGiven that, the value of a(9) is 29524, which is extremely close to 3^8 = 6561, but much larger because we're adding 1 each time. Wait, actually, 3^8 is 6561, but our a(9) is 29524, which is 29524 / 6561 \\u2248 4.5 times larger. So, the additive 1s have accumulated over the 8 iterations.\\n\\nBut regardless, since we've computed each layer step by step and the result is 29524, which when multiplied by 3 gives 88,572, I think that's the correct answer. Unless I made a multiplication error when calculating 3*29524.\\n\\nLet me double-check that multiplication.\\n\\n29524 * 3:\\n\\n20,000 * 3 = 60,000\\n\\n9,000 * 3 = 27,000\\n\\n500 * 3 = 1,500\\n\\n24 * 3 = 72\\n\\nAdding them together:\\n\\n60,000 + 27,000 = 87,000\\n\\n87,000 + 1,500 = 88,500\\n\\n88,500 + 72 = 88,572\\n\\nYes, that seems correct.\\n\\nTherefore, after carefully stepping through each layer and verifying my calculations, I conclude that the value of the given expression is 88,572.\\n\\n**Final Answer**\\nThe value of the expression is \\\\boxed{88572}.\\n</think>\\n\\nTo solve the given nested expression, we start from the innermost parentheses and work our way outwards.\\n\\nThe expression is:\\n\\\\[ 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3))))))))) \\\\]\\n\\nWe compute each layer step by step:\\n\\n1. Compute the innermost layer:\\n   \\\\[ 1 + 3 = 4 \\\\]\\n\\n2. Next layer:\\n   \\\\[ 1 + 3 \\\\times 4 = 1 + 12 = 13 \\\\]\\n\\n3. Next layer:\\n   \\\\[ 1 + 3 \\\\times 13 = 1 + 39 = 40 \\\\]\\n\\n4. Next layer:\\n   \\\\[ 1 + 3 \\\\times 40 = 1 + 120 = 121 \\\\]\\n\\n5. Next layer:\\n   \\\\[ 1 + 3 \\\\times 121 = 1 + 363 = 364 \\\\]\\n\\n6. Next layer:\\n   \\\\[ 1 + 3 \\\\times 364 = 1 + 1092 = 1093 \\\\]\\n\\n7. Next layer:\\n   \\\\[ 1 + 3 \\\\times 1093 = 1 + 3279 = 3280 \\\\]\\n\\n8. Next layer:\\n   \\\\[ 1 + 3 \\\\times 3280 = 1 + 9840 = 9841 \\\\]\\n\\n9. Next layer:\\n   \\\\[ 1 + 3 \\\\times 9841 = 1 + 29523 = 29524 \\\\]\\n\\nFinally, we multiply the result by 3:\\n\\\\[ 3 \\\\times 29524 = 88572 \\\\]\\n\\nThus, the value of the expression is:\\n\\\\[\\n\\\\boxed{88572}\\n\\\\]\",\n",
      "  \"full_cot\": \"Solve this math problem step by step. You MUST put your final answer in \\\\boxed{}. Problem: Compute\\n\\n$3(1+3(1+3(1+3(1+3(1+3(1+3(1+3(1+3(1+3)))))))))$ Solution: \\n<think>\\nAlright, so I've got this math problem here that I need to solve. It's a nested expression with a bunch of threes and parentheses. Let me write it down to get a better look:\\n\\n3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3)))))))))\\n\\nWow, that's a lot of parentheses! Okay, so I need to compute this step by step. I think the best way to approach it is to start from the innermost parentheses and work my way outwards. That usually makes nested expressions easier to handle. Let me try that.\\n\\nFirst, let's identify the innermost part. It seems like the last set of parentheses is (1 + 3). Let me compute that first.\\n\\n1 + 3 = 4\\n\\nOkay, so the innermost part simplifies to 4. Now, replacing that in the expression, the next layer out would be 3(1 + 3(1 + 3(...))). Let me replace the innermost parentheses and see what the next step is.\\n\\nSo after replacing, the expression becomes:\\n\\n3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(4))))))))) )\\n\\nWait, hold on, I think I might have miscounted the layers. Let me go step by step.\\n\\nStarting from the innermost:\\n\\n1. Compute 1 + 3 = 4\\n2. The next layer is 1 + 3(4). Let's compute that.\\n\\nSo, 1 + 3(4) = 1 + 12 = 13\\n\\nWait, that seems too big. Maybe I'm not structuring this correctly. Let me try a different approach. Maybe it's better to evaluate each layer one by one, starting from the innermost.\\n\\nLet me number the layers from the innermost outwards. Let's say the innermost is Layer 1, then Layer 2, and so on, until Layer 9.\\n\\nLayer 1: 1 + 3 = 4\\n\\nLayer 2: 1 + 3(Layer 1) = 1 + 3(4) = 1 + 12 = 13\\n\\nLayer 3: 1 + 3(Layer 2) = 1 + 3(13) = 1 + 39 = 40\\n\\nLayer 4: 1 + 3(Layer 3) = 1 + 3(40) = 1 + 120 = 121\\n\\nLayer 5: 1 + 3(Layer 4) = 1 + 3(121) = 1 + 363 = 364\\n\\nLayer 6: 1 + 3(Layer 5) = 1 + 3(364) = 1 + 1092 = 1093\\n\\nLayer 7: 1 + 3(Layer 6) = 1 + 3(1093) = 1 + 3279 = 3280\\n\\nLayer 8: 1 + 3(Layer 7) = 1 + 3(3280) = 1 + 9840 = 9841\\n\\nLayer 9: 1 + 3(Layer 8) = 1 + 3(9841) = 1 + 29523 = 29524\\n\\nWait, but the original expression starts with a 3 multiplied by all of that. So, the entire expression is 3 times the result of Layer 9.\\n\\nSo, 3 * 29524 = ?\\n\\nLet me compute that.\\n\\n29524 * 3:\\n\\nFirst, 20000 * 3 = 60000\\n\\nThen, 9000 * 3 = 27000\\n\\n500 * 3 = 1500\\n\\n24 * 3 = 72\\n\\nAdding those together:\\n\\n60000 + 27000 = 87000\\n\\n87000 + 1500 = 88500\\n\\n88500 + 72 = 88572\\n\\nSo, the total is 88,572.\\n\\nWait, but let me double-check my calculations because that seems quite large. Maybe I made a mistake in one of the layers.\\n\\nLet me go back and verify each layer step by step.\\n\\nLayer 1: 1 + 3 = 4 \\u2714\\ufe0f\\n\\nLayer 2: 1 + 3*4 = 1 + 12 = 13 \\u2714\\ufe0f\\n\\nLayer 3: 1 + 3*13 = 1 + 39 = 40 \\u2714\\ufe0f\\n\\nLayer 4: 1 + 3*40 = 1 + 120 = 121 \\u2714\\ufe0f\\n\\nLayer 5: 1 + 3*121 = 1 + 363 = 364 \\u2714\\ufe0f\\n\\nLayer 6: 1 + 3*364 = 1 + 1092 = 1093 \\u2714\\ufe0f\\n\\nLayer 7: 1 + 3*1093 = 1 + 3279 = 3280 \\u2714\\ufe0f\\n\\nLayer 8: 1 + 3*3280 = 1 + 9840 = 9841 \\u2714\\ufe0f\\n\\nLayer 9: 1 + 3*9841 = 1 + 29523 = 29524 \\u2714\\ufe0f\\n\\nThen, the entire expression is 3*29524 = 88,572 \\u2714\\ufe0f\\n\\nHmm, that seems consistent. Maybe I didn't make a mistake. Let me think if there's another way to approach this problem.\\n\\nAlternatively, maybe I can recognize a pattern or formula that applies here. Let me see.\\n\\nLooking at the expression, each layer is of the form 1 + 3*(previous layer). So, starting from the innermost, each layer is 1 + 3 times the previous. This is a recursive sequence where each term is defined as a(n) = 1 + 3*a(n-1), with a(1) = 4.\\n\\nWait, actually, in my earlier layers, a(1) was 4, which is 1 + 3. Then a(2) was 13, which is 1 + 3*4, and so on.\\n\\nSo, this is a linear recurrence relation. Maybe I can find a closed-form formula for a(n) and then compute a(9), then multiply by 3.\\n\\nThe recurrence is a(n) = 1 + 3*a(n-1), with a(1) = 4.\\n\\nThis is a nonhomogeneous linear recurrence relation. The general solution can be found by solving the homogeneous part and finding a particular solution.\\n\\nThe homogeneous recurrence is a(n) = 3*a(n-1), which has the solution a(n) = C*3^n.\\n\\nFor the particular solution, since the nonhomogeneous term is constant (1), we can assume a constant particular solution, say a_p.\\n\\nPlugging into the recurrence: a_p = 1 + 3*a_p\\n\\nWait, that gives a_p = 1 + 3*a_p, which simplifies to 0 = 1, which is impossible. So, that suggests that our assumption of a constant particular solution is invalid. Maybe we need to try a linear particular solution, like a_p = k*n.\\n\\nLet me try that. Let a_p = k*n.\\n\\nThen, plugging into the recurrence:\\n\\nk*n = 1 + 3*k*(n-1)\\n\\nSimplify:\\n\\nk*n = 1 + 3k*n - 3k\\n\\nBring all terms to one side:\\n\\nk*n - 3k*n + 3k - 1 = 0\\n\\n-2k*n + 3k - 1 = 0\\n\\nFactor out k:\\n\\nk*(-2n + 3) - 1 = 0\\n\\nThis equation needs to hold for all n, which implies that the coefficients of like terms must be equal on both sides. But since it's a function of n, the coefficients of n and the constants must separately match.\\n\\nLooking at the equation:\\n\\n-2k*n + 3k - 1 = 0\\n\\nThis can be rewritten as:\\n\\n(-2k)n + (3k - 1) = 0\\n\\nFor this to be true for all n, the coefficient of n must be zero, and the constant term must also be zero.\\n\\nSo,\\n\\n-2k = 0  \\u21d2  k = 0\\n\\nAnd,\\n\\n3k - 1 = 0  \\u21d2  3*0 - 1 = -1 \\u2260 0\\n\\nThis is a contradiction, meaning our assumption of a linear particular solution is also invalid. Maybe we need to try a quadratic particular solution, a_p = k*n^2.\\n\\nLet's try that.\\n\\na_p = k*n^2\\n\\nPlugging into the recurrence:\\n\\nk*n^2 = 1 + 3*k*(n-1)^2\\n\\nExpand the right-hand side:\\n\\nk*n^2 = 1 + 3k*(n^2 - 2n + 1)\\n\\nk*n^2 = 1 + 3k*n^2 - 6k*n + 3k\\n\\nBring all terms to the left-hand side:\\n\\nk*n^2 - 3k*n^2 + 6k*n - 3k - 1 = 0\\n\\n(-2k)n^2 + 6k*n - 3k - 1 = 0\\n\\nFor this to hold for all n, the coefficients of each power of n must be zero.\\n\\nSo,\\n\\n-2k = 0  \\u21d2  k = 0\\n\\n6k = 0  \\u21d2  k = 0\\n\\n-3k - 1 = 0  \\u21d2  -0 -1 = -1 \\u2260 0\\n\\nAgain, a contradiction. So, a quadratic particular solution also doesn't work. Maybe I need to try a particular solution of the form a_p = k*n + m, but since that didn't work, maybe I need a different approach.\\n\\nAlternatively, since the recurrence is a(n) = 1 + 3*a(n-1), with a(1) = 4, maybe I can compute the terms iteratively, which I already did earlier, and it gave me a(9) = 29524, leading to 3*29524 = 88,572.\\n\\nGiven that the iterative method is straightforward and I've double-checked each layer, perhaps it's correct. Alternatively, maybe I can look for a pattern in the terms.\\n\\nLet me list out the layers again:\\n\\na(1) = 4\\n\\na(2) = 13\\n\\na(3) = 40\\n\\na(4) = 121\\n\\na(5) = 364\\n\\na(6) = 1093\\n\\na(7) = 3280\\n\\na(8) = 9841\\n\\na(9) = 29524\\n\\nLooking at these numbers: 4, 13, 40, 121, 364, 1093, 3280, 9841, 29524.\\n\\nI notice that each term is approximately 3 times the previous term. For example:\\n\\n13 / 4 = 3.25\\n\\n40 / 13 \\u2248 3.077\\n\\n121 / 40 = 3.025\\n\\n364 / 121 \\u2248 3.008\\n\\n1093 / 364 \\u2248 3.0027\\n\\n3280 / 1093 \\u2248 3.0009\\n\\n9841 / 3280 \\u2248 3.0003\\n\\n29524 / 9841 \\u2248 3.0001\\n\\nSo, as n increases, the ratio approaches 3. That makes sense because the recurrence is a(n) = 1 + 3*a(n-1), so for large n, the 1 becomes negligible, and a(n) \\u2248 3*a(n-1).\\n\\nTherefore, a(n) grows roughly like 3^(n-1). But since we have that constant 1 added each time, it's slightly less than 3^(n-1). However, for n=9, the difference is minimal, as we saw the ratio is already very close to 3.\\n\\nGiven that, the value of a(9) is 29524, which is extremely close to 3^8 = 6561, but much larger because we're adding 1 each time. Wait, actually, 3^8 is 6561, but our a(9) is 29524, which is 29524 / 6561 \\u2248 4.5 times larger. So, the additive 1s have accumulated over the 8 iterations.\\n\\nBut regardless, since we've computed each layer step by step and the result is 29524, which when multiplied by 3 gives 88,572, I think that's the correct answer. Unless I made a multiplication error when calculating 3*29524.\\n\\nLet me double-check that multiplication.\\n\\n29524 * 3:\\n\\n20,000 * 3 = 60,000\\n\\n9,000 * 3 = 27,000\\n\\n500 * 3 = 1,500\\n\\n24 * 3 = 72\\n\\nAdding them together:\\n\\n60,000 + 27,000 = 87,000\\n\\n87,000 + 1,500 = 88,500\\n\\n88,500 + 72 = 88,572\\n\\nYes, that seems correct.\\n\\nTherefore, after carefully stepping through each layer and verifying my calculations, I conclude that the value of the given expression is 88,572.\\n\\n**Final Answer**\\nThe value of the expression is \\\\boxed{88572}.\\n</think>\\n\\nTo solve the given nested expression, we start from the innermost parentheses and work our way outwards.\\n\\nThe expression is:\\n\\\\[ 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3(1 + 3))))))))) \\\\]\\n\\nWe compute each layer step by step:\\n\\n1. Compute the innermost layer:\\n   \\\\[ 1 + 3 = 4 \\\\]\\n\\n2. Next layer:\\n   \\\\[ 1 + 3 \\\\times 4 = 1 + 12 = 13 \\\\]\\n\\n3. Next layer:\\n   \\\\[ 1 + 3 \\\\times 13 = 1 + 39 = 40 \\\\]\\n\\n4. Next layer:\\n   \\\\[ 1 + 3 \\\\times 40 = 1 + 120 = 121 \\\\]\\n\\n5. Next layer:\\n   \\\\[ 1 + 3 \\\\times 121 = 1 + 363 = 364 \\\\]\\n\\n6. Next layer:\\n   \\\\[ 1 + 3 \\\\times 364 = 1 + 1092 = 1093 \\\\]\\n\\n7. Next layer:\\n   \\\\[ 1 + 3 \\\\times 1093 = 1 + 3279 = 3280 \\\\]\\n\\n8. Next layer:\\n   \\\\[ 1 + 3 \\\\times 3280 = 1 + 9840 = 9841 \\\\]\\n\\n9. Next layer:\\n   \\\\[ 1 + 3 \\\\times 9841 = 1 + 29523 = 29524 \\\\]\\n\\nFinally, we multiply the result by 3:\\n\\\\[ 3 \\\\times 29524 = 88572 \\\\]\\n\\nThus, the value of the expression is:\\n\\\\[\\n\\\\boxed{88572}\\n\\\\]\",\n",
      "  \"answer\": \"88572\",\n",
      "  \"is_correct\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the base directory for the problem files\n",
    "problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution/problem_330\"\n",
    "#math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution/problem_330/base_solution.json\n",
    "\n",
    "# The directory containing the solution file\n",
    "# chunk_dir = \"chunk_0\"\n",
    "# The name of the solution file\n",
    "chunk_filename = \"base_solution.json\"\n",
    "\n",
    "# Construct the full path to the solutions.json file\n",
    "chunk_path = os.path.join(problem_dir, chunk_filename)\n",
    "\n",
    "try:\n",
    "    with open(chunk_path, 'r') as f:\n",
    "        # Load the JSON content from the file\n",
    "        chunk_data = json.load(f)\n",
    "\n",
    "    print(f\"Content of {chunk_filename}:\")\n",
    "    # Pretty-print the JSON object for better readability\n",
    "    print(json.dumps(chunk_data, indent=2))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {chunk_path} was not found.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: The file {chunk_path} is not a valid JSON file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97179a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = chunk_data[\"full_cot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46094640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" # Or any other suitable model\n",
    "\n",
    "mname = model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Important: Add a pad token if the tokenizer doesn't have one, especially for decoder models.\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78fc8dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model with `output_attentions=True`\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96613856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Access attention weights\n",
    "attention_weights = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d732f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac0a4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Head 0: [1.         1.0004883  0.9995117  ... 1.0003573  0.99996865 1.0009204 ]\n",
      "Layer 0, Head 1: [1.        1.0014648 0.998291  ... 1.0003076 1.0000572 0.9999357]\n",
      "Layer 0, Head 2: [1.         1.0014648  0.9995117  ... 0.99974346 1.0010469  0.99994653]\n",
      "Layer 0, Head 3: [1.        1.        1.        ... 0.9999404 1.0007172 0.9993855]\n",
      "Layer 0, Head 4: [1.        1.0004883 0.9991455 ... 1.0002215 1.0000865 1.0004596]\n",
      "Layer 0, Head 5: [1.         0.99902344 1.0014648  ... 0.99995345 1.000308   0.9999411 ]\n",
      "Layer 0, Head 6: [1.        1.        1.        ... 1.0001962 0.9997057 1.0000097]\n",
      "Layer 0, Head 7: [1.        1.0009766 0.9984131 ... 1.0003548 1.0000226 1.0000203]\n",
      "Layer 0, Head 8: [1.         1.0009766  1.0004883  ... 0.9999478  0.99983406 0.99997604]\n",
      "Layer 0, Head 9: [1.        0.998291  0.9992676 ... 0.9997413 0.9998176 1.000044 ]\n",
      "Layer 0, Head 10: [1.         1.0009766  1.0004883  ... 1.0002431  0.9985289  0.99931955]\n",
      "Layer 0, Head 11: [1.         0.99902344 1.0009766  ... 1.0000291  0.9998615  0.9999886 ]\n",
      "Layer 0, Head 12: [1.         0.99902344 1.         ... 0.9999856  1.0001895  1.0000347 ]\n",
      "Layer 0, Head 13: [1.         1.0019531  1.0008545  ... 0.999981   0.99987555 0.9999366 ]\n",
      "Layer 0, Head 14: [1.         1.0014648  0.99920654 ... 0.99982077 0.999992   1.0000539 ]\n",
      "Layer 0, Head 15: [1.         0.99902344 1.0002441  ... 1.0000092  0.99996996 0.9998919 ]\n",
      "Layer 0, Head 16: [1.         0.9980774  0.99908066 ... 0.99990684 1.000156   0.9990719 ]\n",
      "Layer 0, Head 17: [1.        1.0017624 1.0017548 ... 0.9999208 0.9983418 1.0009459]\n",
      "Layer 0, Head 18: [1.         1.0011826  0.99944305 ... 1.00016    1.000936   0.9994422 ]\n",
      "Layer 0, Head 19: [1.         1.0010605  1.0015984  ... 0.9999831  0.99959147 0.9998576 ]\n",
      "Layer 0, Head 20: [1.         1.0019531  1.0004883  ... 1.000886   0.99971175 0.999591  ]\n",
      "Layer 0, Head 21: [1.         0.99835205 0.9996948  ... 1.000476   0.999982   1.0000076 ]\n",
      "Layer 0, Head 22: [1.         0.99823    1.0019226  ... 1.0001382  0.99983376 0.99994516]\n",
      "Layer 0, Head 23: [1.        0.9980469 1.0014648 ... 0.9999254 0.9999193 0.9999496]\n",
      "Layer 0, Head 24: [1.         1.0014648  0.99975586 ... 1.0002003  0.99976885 0.99991924]\n",
      "Layer 0, Head 25: [1.         1.         0.9984131  ... 0.99994767 1.0000376  1.0000687 ]\n",
      "Layer 0, Head 26: [1.        1.        0.9987793 ... 1.0001101 1.0002477 0.9999223]\n",
      "Layer 0, Head 27: [1.        1.0019531 0.9980469 ... 1.0000284 1.0000579 1.000144 ]\n",
      "Layer 0, Head 28: [1.        1.0013351 1.0005693 ... 1.0003374 0.9991362 1.0009918]\n",
      "Layer 0, Head 29: [1.         1.0001783  1.0009189  ... 1.0012683  0.99998194 1.0010579 ]\n",
      "Layer 0, Head 30: [1.         0.99902344 1.0003052  ... 1.0007565  1.0004193  1.0006704 ]\n",
      "Layer 0, Head 31: [1.        1.0000001 1.0000002 ... 1.0006726 1.0011423 1.0015074]\n",
      "Layer 1, Head 0: [1.         0.9992981  1.0006714  ... 1.0015299  1.0015929  0.99950135]\n",
      "Layer 1, Head 1: [1.         0.99887085 1.         ... 0.99983984 0.99966836 0.99996203]\n",
      "Layer 1, Head 2: [1.         0.99884033 1.0012207  ... 1.0006945  0.99916    1.0018888 ]\n",
      "Layer 1, Head 3: [1.         1.0018387  1.0001221  ... 0.9991108  0.99813676 0.99929386]\n",
      "Layer 1, Head 4: [1.         0.9985962  0.99890137 ... 0.9989548  0.99914974 1.0005206 ]\n",
      "Layer 1, Head 5: [1.        0.9986572 1.0005493 ... 1.0018264 1.0002826 1.0020251]\n",
      "Layer 1, Head 6: [1.         0.99897766 0.9987488  ... 0.99894965 1.0003284  0.99951243]\n",
      "Layer 1, Head 7: [1.        1.001709  0.9981842 ... 1.0006877 1.00028   0.9998284]\n",
      "Layer 1, Head 8: [1.        1.0014648 0.9994507 ... 1.0015161 1.0007948 0.9992963]\n",
      "Layer 1, Head 9: [1.         0.99853516 1.0003662  ... 1.0010597  1.0010729  0.99858683]\n",
      "Layer 1, Head 10: [1.         0.99975586 1.0002441  ... 1.0018386  0.99868536 0.9990993 ]\n",
      "Layer 1, Head 11: [1.         0.9995117  0.998291   ... 0.99939215 0.99817836 1.0000163 ]\n",
      "Layer 1, Head 12: [1.         0.99871826 1.0007477  ... 1.000644   0.9980483  0.99847305]\n",
      "Layer 1, Head 13: [1.         0.9998779  0.99816895 ... 1.0006361  1.0005915  1.001084  ]\n",
      "Layer 1, Head 14: [1.         1.0003052  0.99806213 ... 1.0015864  0.999172   1.001719  ]\n",
      "Layer 1, Head 15: [1.         1.0016708  0.99824524 ... 1.0010529  1.000935   1.0017154 ]\n",
      "Layer 1, Head 16: [1.        0.9992676 0.9980774 ... 0.9999757 1.0003995 1.0010186]\n",
      "Layer 1, Head 17: [1.        1.0008698 1.0007324 ... 1.0016075 0.9981849 1.0016778]\n",
      "Layer 1, Head 18: [1.        1.0014648 1.001831  ... 1.000082  1.0003607 0.9988018]\n",
      "Layer 1, Head 19: [1.        1.0007362 1.0008392 ... 1.0008746 1.0010505 1.0017357]\n",
      "Layer 1, Head 20: [1.         1.0019531  1.0014648  ... 1.0004818  0.99862707 0.99863636]\n",
      "Layer 1, Head 21: [1.         0.9992981  0.99990845 ... 0.99933314 0.9995579  1.0006971 ]\n",
      "Layer 1, Head 22: [1.         1.0000916  0.99822235 ... 1.0010684  1.0002509  1.0010455 ]\n",
      "Layer 1, Head 23: [1.         0.9989624  0.99833083 ... 1.0019801  1.0001292  1.000447  ]\n",
      "Layer 1, Head 24: [1.         0.99938965 1.0013275  ... 0.99919987 0.9993789  1.0011193 ]\n",
      "Layer 1, Head 25: [1.        1.0004883 0.9984741 ... 1.001437  1.0003983 1.0000036]\n",
      "Layer 1, Head 26: [1.         1.0004883  1.0019531  ... 0.9984584  1.0017574  0.99892664]\n",
      "Layer 1, Head 27: [1.         1.0006104  0.99816895 ... 0.9985131  0.9999388  0.998115  ]\n",
      "Layer 1, Head 28: [1.        1.0015869 0.9991455 ... 1.0012898 1.0004423 1.0006366]\n",
      "Layer 1, Head 29: [1.        0.9992676 1.0018921 ... 0.9980686 1.0004232 0.9983289]\n",
      "Layer 1, Head 30: [1.        1.0002441 0.9998169 ... 1.0000668 1.0001559 1.0012783]\n",
      "Layer 1, Head 31: [1.        1.0009766 0.9987793 ... 1.001277  1.0008265 1.0009317]\n",
      "Layer 2, Head 0: [1.        1.0011673 0.9990845 ... 0.9997285 0.9984803 1.0015479]\n",
      "Layer 2, Head 1: [1.         1.000576   1.0016632  ... 1.0005407  0.9992533  0.99932885]\n",
      "Layer 2, Head 2: [1.        1.0005722 1.0000916 ... 1.0015029 1.0011723 1.0009956]\n",
      "Layer 2, Head 3: [1.         0.99812317 1.0013428  ... 1.0016553  0.9993369  1.0005047 ]\n",
      "Layer 2, Head 4: [1.         0.9992676  0.99884033 ... 0.9993862  0.99921435 0.9991298 ]\n",
      "Layer 2, Head 5: [1.         0.99838257 1.000061   ... 1.0014762  1.001642   1.0003402 ]\n",
      "Layer 2, Head 6: [1.        0.9987793 1.0007172 ... 0.9996339 0.9997572 1.0006365]\n",
      "Layer 2, Head 7: [1.         1.0003662  1.0005493  ... 0.99989    1.0005592  0.99852943]\n",
      "Layer 2, Head 8: [1.         0.9990082  1.0016174  ... 0.9982551  0.99848485 0.99932283]\n",
      "Layer 2, Head 9: [1.        1.0007019 1.0001831 ... 1.0018654 0.9998688 0.9989007]\n",
      "Layer 2, Head 10: [1.        1.0011063 1.0002136 ... 0.9982737 0.9986346 1.0002035]\n",
      "Layer 2, Head 11: [1.        1.001442  0.9985657 ... 1.0012468 1.0013227 0.9986929]\n",
      "Layer 2, Head 12: [1.         0.99850464 0.9990692  ... 0.99877036 1.0014086  0.9991093 ]\n",
      "Layer 2, Head 13: [1.        0.9980469 0.9989319 ... 0.9990793 1.0002724 0.9990403]\n",
      "Layer 2, Head 14: [1.         1.0011139  0.9992981  ... 0.99987894 1.0009717  0.99829143]\n",
      "Layer 2, Head 15: [1.         0.9990845  1.0000916  ... 0.99889874 1.0000511  1.0014219 ]\n",
      "Layer 2, Head 16: [1.        1.0007629 0.9990845 ... 0.9987758 0.9998543 1.0000365]\n",
      "Layer 2, Head 17: [1.        1.0015869 1.0004883 ... 1.0002061 1.0014478 1.0012014]\n",
      "Layer 2, Head 18: [1.        1.0005188 1.000885  ... 1.0015733 1.0018268 1.0001829]\n",
      "Layer 2, Head 19: [1.         0.9992676  0.99935913 ... 1.0005023  1.000567   1.0015543 ]\n",
      "Layer 2, Head 20: [1.        1.000946  1.0000114 ... 0.9986142 0.9990803 1.0008051]\n",
      "Layer 2, Head 21: [1.         1.000927   0.99957275 ... 0.99899256 1.0017773  0.99888366]\n",
      "Layer 2, Head 22: [1.         0.9995575  1.0001907  ... 1.0007217  1.0014921  0.99820644]\n",
      "Layer 2, Head 23: [1.         1.0009766  0.99902344 ... 1.0016488  1.0006304  0.998787  ]\n",
      "Layer 2, Head 24: [1.         1.0012817  1.0014648  ... 1.0018013  0.99867356 0.9980626 ]\n",
      "Layer 2, Head 25: [1.         1.0013657  1.0014954  ... 1.0008942  1.0008924  0.99953586]\n",
      "Layer 2, Head 26: [1.        0.9987793 1.000946  ... 1.0012453 0.9998884 1.0006155]\n",
      "Layer 2, Head 27: [1.        1.0004883 0.9984131 ... 0.9980233 1.0000145 1.0017499]\n",
      "Layer 2, Head 28: [1.        1.0013428 1.0018005 ... 1.0013942 0.9985659 0.9983677]\n",
      "Layer 2, Head 29: [1.        1.0007782 1.0010757 ... 1.0002851 1.0007464 1.0016611]\n",
      "Layer 2, Head 30: [1.         1.0016785  0.99902344 ... 0.9980597  1.0003089  0.99972963]\n",
      "Layer 2, Head 31: [1.         1.         0.99832153 ... 1.0014144  0.9981198  0.99912465]\n",
      "Layer 3, Head 0: [1.        1.0015564 1.0006256 ... 0.9989179 1.0007831 1.0001758]\n",
      "Layer 3, Head 1: [1.         0.99902344 1.0002441  ... 1.0018545  0.99922854 1.0000063 ]\n",
      "Layer 3, Head 2: [1.        1.0005493 1.0006714 ... 1.0019507 0.9998284 1.0003978]\n",
      "Layer 3, Head 3: [1.         1.0003662  0.9996643  ... 1.0009279  1.0007432  0.99941933]\n",
      "Layer 3, Head 4: [1.        1.0006409 0.9987793 ... 0.99825   0.9985573 1.0018388]\n",
      "Layer 3, Head 5: [1.         0.99975586 1.0011597  ... 0.9999346  1.0006372  1.0010633 ]\n",
      "Layer 3, Head 6: [1.         0.9984131  1.001709   ... 1.0008876  0.99851125 1.0010121 ]\n",
      "Layer 3, Head 7: [1.         0.99902344 0.9988556  ... 0.99995965 1.0002183  1.0013741 ]\n",
      "Layer 3, Head 8: [1.         0.9987793  1.0008545  ... 0.99859524 1.0013394  0.99988985]\n",
      "Layer 3, Head 9: [1.         0.9998169  0.99819946 ... 1.001892   0.9987896  0.9983808 ]\n",
      "Layer 3, Head 10: [1.        0.999527  1.0007935 ... 1.0022546 1.0021732 0.9996522]\n",
      "Layer 3, Head 11: [1.        0.9992676 0.9996643 ... 1.0001158 1.0017391 0.9991606]\n",
      "Layer 3, Head 12: [1.         1.0009842  1.0014343  ... 0.9996548  0.9986026  0.99919194]\n",
      "Layer 3, Head 13: [1.         1.0010681  0.99967957 ... 1.0016533  0.99970675 0.99840826]\n",
      "Layer 3, Head 14: [1.        0.9989319 0.9993286 ... 1.0003599 1.0011442 1.0007268]\n",
      "Layer 3, Head 15: [1.         1.0018387  0.9994507  ... 1.0016751  1.0014344  0.99865186]\n",
      "Layer 3, Head 16: [1.         1.0004272  1.         ... 0.99881923 0.9989438  0.9987186 ]\n",
      "Layer 3, Head 17: [1.         0.9987793  0.99798584 ... 1.0015363  1.000389   0.9981046 ]\n",
      "Layer 3, Head 18: [1.         1.0006104  0.9992676  ... 0.99896526 1.0005476  0.998768  ]\n",
      "Layer 3, Head 19: [1.         1.0019531  1.0012207  ... 0.99869615 0.9987019  0.9994352 ]\n",
      "Layer 3, Head 20: [1.         1.0002441  1.0003662  ... 0.99970484 0.99840283 0.998384  ]\n",
      "Layer 3, Head 21: [1.        1.0005493 1.0010223 ... 1.0016849 1.0001175 1.001378 ]\n",
      "Layer 3, Head 22: [1.         1.0004883  0.99938965 ... 0.9982345  0.99872607 1.0001543 ]\n",
      "Layer 3, Head 23: [1.         0.99975586 1.0014648  ... 1.0005659  1.0006146  1.0001488 ]\n",
      "Layer 3, Head 24: [1.         1.0011597  0.998291   ... 0.99851733 1.0011998  0.9980787 ]\n",
      "Layer 3, Head 25: [1.         0.99838257 1.0003662  ... 0.99843925 0.99874866 0.9982013 ]\n",
      "Layer 3, Head 26: [1.         1.0007935  0.9984131  ... 0.99875164 0.99823594 0.9991902 ]\n",
      "Layer 3, Head 27: [1.         0.99902344 0.99972534 ... 0.9994026  1.001616   0.998906  ]\n",
      "Layer 3, Head 28: [1.         1.0004272  0.99853516 ... 1.0002398  0.99895    1.0003794 ]\n",
      "Layer 3, Head 29: [1.         1.001709   0.9986572  ... 0.99826515 1.0003893  1.0017376 ]\n",
      "Layer 3, Head 30: [1.        1.0018921 0.9984131 ... 0.9988781 1.0019662 1.0017244]\n",
      "Layer 3, Head 31: [1.         0.9986572  0.99816895 ... 1.0014066  0.998707   0.99901193]\n",
      "Layer 4, Head 0: [1.         0.99957275 1.0004578  ... 1.0006645  0.9992508  1.0014937 ]\n",
      "Layer 4, Head 1: [1.         0.99861145 0.9984741  ... 1.0017246  0.99850893 0.99893284]\n",
      "Layer 4, Head 2: [1.         1.0010071  1.         ... 1.0016004  0.9978759  0.99945533]\n",
      "Layer 4, Head 3: [1.         1.0015869  0.99938965 ... 1.0014445  1.0008024  1.0000044 ]\n",
      "Layer 4, Head 4: [1.         0.998291   1.0008545  ... 0.9986688  0.9998164  0.99992794]\n",
      "Layer 4, Head 5: [1.         0.99853516 1.0015869  ... 0.9990102  0.99805963 1.0009758 ]\n",
      "Layer 4, Head 6: [1.         0.99954224 0.998291   ... 0.99976766 1.0019499  1.000321  ]\n",
      "Layer 4, Head 7: [1.         0.99902344 1.0010376  ... 1.000078   1.0010829  1.0017115 ]\n",
      "Layer 4, Head 8: [1.         0.99853516 0.99816895 ... 1.0019041  0.9980653  1.001133  ]\n",
      "Layer 4, Head 9: [1.         1.001709   0.998291   ... 0.9999751  0.99971294 1.0018244 ]\n",
      "Layer 4, Head 10: [1.         0.9995117  1.001831   ... 0.9988174  0.99977636 0.99845886]\n",
      "Layer 4, Head 11: [1.        1.        1.0010986 ... 1.0000839 1.0003461 0.9987304]\n",
      "Layer 4, Head 12: [1.        0.9987793 0.9992676 ... 1.0001378 1.0017853 1.0009531]\n",
      "Layer 4, Head 13: [1.         1.0019531  1.0009155  ... 0.99839175 1.0018262  1.001696  ]\n",
      "Layer 4, Head 14: [1.         0.99975586 0.9984741  ... 1.0004765  1.0006641  0.9988398 ]\n",
      "Layer 4, Head 15: [1.        1.001831  0.9989624 ... 1.00125   1.0004522 1.0006504]\n",
      "Layer 4, Head 16: [1.         1.0009766  0.9997101  ... 0.9999243  1.0004516  0.99920464]\n",
      "Layer 4, Head 17: [1.         0.99890137 0.9997864  ... 1.000547   1.0006834  1.0019329 ]\n",
      "Layer 4, Head 18: [1.         0.99884033 1.0010681  ... 1.0007559  0.9994862  0.9995669 ]\n",
      "Layer 4, Head 19: [1.         1.001709   1.0003052  ... 0.99963886 1.0008415  0.99956554]\n",
      "Layer 4, Head 20: [1.         0.99819946 0.99889374 ... 1.0002962  0.9998754  0.9999949 ]\n",
      "Layer 4, Head 21: [1.         0.99871826 1.0004272  ... 0.9987022  1.001206   1.0001574 ]\n",
      "Layer 4, Head 22: [1.         0.99993896 1.0002441  ... 1.0000218  0.9993073  0.99873585]\n",
      "Layer 4, Head 23: [1.        1.0013123 0.9988861 ... 0.9990593 1.0004814 1.0011393]\n",
      "Layer 4, Head 24: [1.         1.0008545  0.99975586 ... 1.0011986  0.99958205 1.000916  ]\n",
      "Layer 4, Head 25: [1.        1.0009766 1.0007935 ... 0.9991998 0.9991931 1.0003724]\n",
      "Layer 4, Head 26: [1.        1.        1.0004883 ... 1.0009063 1.0012608 1.0018338]\n",
      "Layer 4, Head 27: [1.         1.0019531  0.99853516 ... 1.002042   0.99922943 0.9993819 ]\n",
      "Layer 4, Head 28: [1.        1.        1.        ... 1.0016577 1.0002229 0.9983882]\n",
      "Layer 4, Head 29: [1.         0.99931335 0.9984436  ... 0.9987007  1.0002903  1.0001192 ]\n",
      "Layer 4, Head 30: [1.        0.9998779 1.0014038 ... 0.9995892 1.0005815 1.000195 ]\n",
      "Layer 4, Head 31: [1.        1.0007629 1.0010986 ... 1.0001614 1.0002775 1.0013297]\n",
      "Layer 5, Head 0: [1.        0.9989624 1.0003662 ... 0.9983765 0.9986753 1.0014334]\n",
      "Layer 5, Head 1: [1.        0.9987793 1.0019226 ... 1.0013475 1.0015708 1.0006127]\n",
      "Layer 5, Head 2: [1.         1.0014648  1.0012817  ... 0.99845386 1.0009751  1.0018351 ]\n",
      "Layer 5, Head 3: [1.         0.9998169  1.000061   ... 0.99954873 0.9994564  1.0016975 ]\n",
      "Layer 5, Head 4: [1.        1.001709  1.0000305 ... 1.0004154 1.000795  1.0007738]\n",
      "Layer 5, Head 5: [1.         0.9995117  0.99931717 ... 1.0003719  1.0000505  1.0008999 ]\n",
      "Layer 5, Head 6: [1.        1.0009155 0.9990845 ... 1.000108  0.999434  1.0013597]\n",
      "Layer 5, Head 7: [1.         0.9996948  1.0012207  ... 0.99963087 0.99950016 1.0013837 ]\n",
      "Layer 5, Head 8: [1.         0.99864197 0.99879456 ... 0.9990687  0.9990517  0.99907774]\n",
      "Layer 5, Head 9: [1.        0.9984894 1.0002251 ... 1.000572  1.0007765 1.0011611]\n",
      "Layer 5, Head 10: [1.         0.99845886 0.99863434 ... 1.001569   0.9979347  0.99810743]\n",
      "Layer 5, Head 11: [1.        1.00177   1.0001221 ... 0.9999746 0.9980731 1.000622 ]\n",
      "Layer 5, Head 12: [1.         1.0012207  1.         ... 1.0007031  0.99863684 1.000636  ]\n",
      "Layer 5, Head 13: [1.        1.0007324 1.001709  ... 1.0003178 0.9997433 0.999392 ]\n",
      "Layer 5, Head 14: [1.        0.9994507 1.0019531 ... 0.9993359 1.0006719 1.000469 ]\n",
      "Layer 5, Head 15: [1.        1.0019531 0.9980774 ... 0.9983456 1.0001005 0.9994128]\n",
      "Layer 5, Head 16: [1.        0.9980469 0.9995117 ... 1.0012307 0.99865   1.0005276]\n",
      "Layer 5, Head 17: [1.         0.99975586 1.0012207  ... 1.0016559  1.0009348  0.99876714]\n",
      "Layer 5, Head 18: [1.        1.0010986 1.0004272 ... 0.9989052 1.0009072 1.0001627]\n",
      "Layer 5, Head 19: [1.         0.99853516 0.99938965 ... 0.9998003  1.0003448  0.9994255 ]\n",
      "Layer 5, Head 20: [1.         0.99920654 0.9987488  ... 0.99895096 0.9986509  1.0004804 ]\n",
      "Layer 5, Head 21: [1.        0.998291  1.0005493 ... 0.9995495 1.0018756 1.0011855]\n",
      "Layer 5, Head 22: [1.         0.99890137 0.9986572  ... 0.999428   0.9988456  1.0008179 ]\n",
      "Layer 5, Head 23: [1.         0.99957275 1.0006409  ... 1.0001129  1.00035    0.9993793 ]\n",
      "Layer 5, Head 24: [1.         0.99942017 1.0000305  ... 1.001058   0.9984684  1.0015764 ]\n",
      "Layer 5, Head 25: [1.         0.99942017 0.99838257 ... 0.9983181  0.9996039  1.001896  ]\n",
      "Layer 5, Head 26: [1.        1.0004883 0.9986572 ... 1.001599  0.9986686 1.0004643]\n",
      "Layer 5, Head 27: [1.        0.9981079 1.0003357 ... 1.0004692 1.0017657 0.9992888]\n",
      "Layer 5, Head 28: [1.        0.9987793 1.0009766 ... 1.000706  1.0001357 1.0004247]\n",
      "Layer 5, Head 29: [1.         1.0002441  1.0017395  ... 0.9993136  0.99983263 0.9989257 ]\n",
      "Layer 5, Head 30: [1.         0.99887085 0.99884033 ... 0.9981118  1.0020207  0.99808663]\n",
      "Layer 5, Head 31: [1.         1.0014648  1.0001526  ... 1.0023205  0.99933356 0.999817  ]\n",
      "Layer 6, Head 0: [1.        0.9998779 1.0004883 ... 0.9982102 1.0000367 0.999838 ]\n",
      "Layer 6, Head 1: [1.         0.9994507  1.         ... 1.0012105  0.99939036 0.99902874]\n",
      "Layer 6, Head 2: [1.         0.9982147  0.9996948  ... 0.99937785 0.9992829  0.99907935]\n",
      "Layer 6, Head 3: [1.         0.9998779  0.99993896 ... 0.9994222  1.0008838  0.9997442 ]\n",
      "Layer 6, Head 4: [1.        1.001831  0.99823   ... 1.0008059 1.0002462 0.9982838]\n",
      "Layer 6, Head 5: [1.         1.0009766  1.0008545  ... 0.9994929  0.99918324 0.99958897]\n",
      "Layer 6, Head 6: [1.         0.99920654 0.99801636 ... 0.9983351  1.0017741  0.9997363 ]\n",
      "Layer 6, Head 7: [1.        1.0007324 1.0002747 ... 0.9990069 1.0015105 0.9994612]\n",
      "Layer 6, Head 8: [1.         0.9992676  0.9989624  ... 1.0007567  0.9984046  0.99853814]\n",
      "Layer 6, Head 9: [1.         0.99897766 0.999506   ... 0.9982766  1.0003155  1.0014327 ]\n",
      "Layer 6, Head 10: [1.         1.0007324  0.99835205 ... 1.0010321  1.0018867  1.0006557 ]\n",
      "Layer 6, Head 11: [1.        1.0011597 0.9981384 ... 0.9981326 1.0001819 1.0011308]\n",
      "Layer 6, Head 12: [1.         1.0012207  1.0009155  ... 0.9993849  0.99820715 1.0010278 ]\n",
      "Layer 6, Head 13: [1.        1.0004883 1.0009766 ... 1.0015931 0.9980171 1.0018028]\n",
      "Layer 6, Head 14: [1.         1.0009766  0.9995785  ... 0.99953246 0.99915856 0.99919975]\n",
      "Layer 6, Head 15: [1.        0.9998779 0.998291  ... 1.0018206 0.9994491 1.0013323]\n",
      "Layer 6, Head 16: [1.        1.        0.9985657 ... 1.0008578 1.0009432 1.0008314]\n",
      "Layer 6, Head 17: [1.        1.0001221 1.001648  ... 1.000313  0.9983653 1.0011481]\n",
      "Layer 6, Head 18: [1.        1.0014648 1.0012207 ... 1.0010711 1.0008059 1.0004485]\n",
      "Layer 6, Head 19: [1.        0.998291  1.0014648 ... 0.9982041 0.9983275 1.0008469]\n",
      "Layer 6, Head 20: [1.         0.9995117  0.9981079  ... 0.99992764 1.0012836  1.0016749 ]\n",
      "Layer 6, Head 21: [1.         1.0004883  1.0008545  ... 1.0011284  0.99906504 1.000196  ]\n",
      "Layer 6, Head 22: [1.         0.99853516 1.0007629  ... 0.9980314  0.999951   1.0007714 ]\n",
      "Layer 6, Head 23: [1.         1.00177    1.001831   ... 0.99961185 0.9988506  0.99869907]\n",
      "Layer 6, Head 24: [1.         1.0007324  0.9984131  ... 0.99938875 0.9980297  0.9998019 ]\n",
      "Layer 6, Head 25: [1.        0.9984741 1.0004883 ... 1.0004692 0.9982004 0.9982158]\n",
      "Layer 6, Head 26: [1.         1.0018387  0.99863434 ... 0.9985006  0.9998219  0.99836135]\n",
      "Layer 6, Head 27: [1.         1.0000916  0.998291   ... 1.0012671  0.99983764 0.9998678 ]\n",
      "Layer 6, Head 28: [1.         1.0009766  1.0007324  ... 0.9981011  0.99971116 1.0008721 ]\n",
      "Layer 6, Head 29: [1.        0.9987793 1.000473  ... 0.9989675 1.0013511 1.0019507]\n",
      "Layer 6, Head 30: [1.         0.99853516 1.0013428  ... 0.9980204  0.9996625  0.99885833]\n",
      "Layer 6, Head 31: [1.        0.9986572 0.9981632 ... 1.0017463 1.0018561 1.0010931]\n",
      "Layer 7, Head 0: [1.         1.0019531  1.0009766  ... 1.0009382  0.99987423 0.9991    ]\n",
      "Layer 7, Head 1: [1.         1.0004883  1.001709   ... 0.99935704 1.0005172  1.0005074 ]\n",
      "Layer 7, Head 2: [1.         0.9992676  1.0004883  ... 0.99951065 1.0015504  0.99906963]\n",
      "Layer 7, Head 3: [1.        1.0001221 1.0015717 ... 1.0001628 1.0003955 1.0001438]\n",
      "Layer 7, Head 4: [1.         0.9993286  0.9996338  ... 0.9987959  0.99940336 0.9987632 ]\n",
      "Layer 7, Head 5: [1.         1.0009766  0.99853516 ... 1.0018868  1.0011705  1.00148   ]\n",
      "Layer 7, Head 6: [1.         0.99835205 1.0005035  ... 0.9992617  1.0001193  0.9986893 ]\n",
      "Layer 7, Head 7: [1.        0.9989624 0.9989624 ... 0.9999323 0.9994798 1.000337 ]\n",
      "Layer 7, Head 8: [1.         1.0019531  0.99975586 ... 0.99926174 0.9990836  0.9999269 ]\n",
      "Layer 7, Head 9: [1.         0.9992676  0.9994507  ... 0.99931955 0.99810785 1.0006964 ]\n",
      "Layer 7, Head 10: [1.        0.9986572 0.9998169 ... 0.9981096 0.9990344 0.9987504]\n",
      "Layer 7, Head 11: [1.         1.0003662  1.001831   ... 1.0002239  0.99934393 1.0016828 ]\n",
      "Layer 7, Head 12: [1.         0.99816895 0.9986572  ... 1.001219   1.0020946  0.99814844]\n",
      "Layer 7, Head 13: [1.         1.         0.99920654 ... 0.9979701  1.0019133  1.0004388 ]\n",
      "Layer 7, Head 14: [1.         0.99902344 0.99890137 ... 0.99950796 0.99939466 1.000054  ]\n",
      "Layer 7, Head 15: [1.         0.9990845  0.99938965 ... 0.9997386  1.0002216  0.99982476]\n",
      "Layer 7, Head 16: [1.         0.9994507  1.0002441  ... 1.00137    1.0016261  0.99897707]\n",
      "Layer 7, Head 17: [1.         1.0009155  1.0000153  ... 1.0006196  0.99925035 0.9991902 ]\n",
      "Layer 7, Head 18: [1.        1.0014038 0.9988098 ... 1.0000222 0.9986606 1.0016476]\n",
      "Layer 7, Head 19: [1.         1.0004883  1.0014648  ... 1.0000569  1.0018464  0.99965835]\n",
      "Layer 7, Head 20: [1.         0.99853516 1.001831   ... 0.9992518  0.9982602  0.99987733]\n",
      "Layer 7, Head 21: [1.        1.0014648 0.9992676 ... 1.001043  0.9983616 0.9994811]\n",
      "Layer 7, Head 22: [1.         1.0019302  1.0015564  ... 1.0001657  0.99919474 0.9987794 ]\n",
      "Layer 7, Head 23: [1.         0.9995117  0.9996948  ... 0.99972844 0.99917936 0.999793  ]\n",
      "Layer 7, Head 24: [1.         1.0009766  1.0008545  ... 0.99881876 1.0001945  0.99966204]\n",
      "Layer 7, Head 25: [1.        0.9990845 0.9986572 ... 0.9993017 1.0003234 1.0007766]\n",
      "Layer 7, Head 26: [1.         0.9980469  1.0005951  ... 0.9983386  0.99863565 0.9989787 ]\n",
      "Layer 7, Head 27: [1.         0.998291   1.0004578  ... 1.0003247  0.99863696 1.0002031 ]\n",
      "Layer 7, Head 28: [1.        0.998291  1.0010986 ... 1.0011021 1.0001614 0.9997263]\n",
      "Layer 7, Head 29: [1.         1.0015259  0.99868774 ... 1.0010146  1.0006335  1.0013062 ]\n",
      "Layer 7, Head 30: [1.         1.0015869  0.99832153 ... 0.9989422  0.99827117 1.0009481 ]\n",
      "Layer 7, Head 31: [1.         0.99816895 1.0007935  ... 1.0014492  1.0005865  0.999429  ]\n",
      "Layer 8, Head 0: [1.        1.0001221 0.9996643 ... 0.9987023 0.9985559 1.000247 ]\n",
      "Layer 8, Head 1: [1.        1.0015564 0.9990845 ... 1.00039   1.0000321 1.0002602]\n",
      "Layer 8, Head 2: [1.         1.0009766  0.99975586 ... 1.0004879  1.0003462  1.000721  ]\n",
      "Layer 8, Head 3: [1.         0.99823    0.9998169  ... 1.0010846  0.9992085  0.99929047]\n",
      "Layer 8, Head 4: [1.         0.99920654 1.0015259  ... 0.9989791  1.0003626  1.0014659 ]\n",
      "Layer 8, Head 5: [1.        1.0009766 1.0019531 ... 1.0004312 1.0006263 1.0003917]\n",
      "Layer 8, Head 6: [1.        1.0002441 0.9998169 ... 1.0010686 0.9984641 0.999799 ]\n",
      "Layer 8, Head 7: [1.        0.9991455 1.0014038 ... 0.9998255 0.9988606 1.0016727]\n",
      "Layer 8, Head 8: [1.         0.99902344 0.9994507  ... 0.9998154  0.99951833 1.0002055 ]\n",
      "Layer 8, Head 9: [1.         1.0001221  0.99902344 ... 1.000172   1.0005759  0.99939835]\n",
      "Layer 8, Head 10: [1.        0.9984741 1.0000076 ... 1.0001771 1.0005784 1.0005357]\n",
      "Layer 8, Head 11: [1.         0.99890137 0.9994812  ... 0.9993547  1.0005037  1.0003271 ]\n",
      "Layer 8, Head 12: [1.         1.0008545  0.99990845 ... 0.99950194 0.9997543  0.9994439 ]\n",
      "Layer 8, Head 13: [1.         1.0009766  0.99902344 ... 0.99877435 1.0004972  1.0010148 ]\n",
      "Layer 8, Head 14: [1.         0.9987793  0.99879456 ... 0.9980555  1.0015283  0.99986887]\n",
      "Layer 8, Head 15: [1.        0.9986572 0.999115  ... 0.9999829 1.0013386 1.0011072]\n",
      "Layer 8, Head 16: [1.         0.99902344 1.0012207  ... 1.0001729  0.9979191  0.9991705 ]\n",
      "Layer 8, Head 17: [1.         0.99871826 0.9990845  ... 1.0016288  1.0017556  1.000144  ]\n",
      "Layer 8, Head 18: [1.        1.0017395 0.9998169 ... 1.0011624 0.9987344 1.0017588]\n",
      "Layer 8, Head 19: [1.         0.9987793  1.0014038  ... 0.9983903  1.0012343  0.99939483]\n",
      "Layer 8, Head 20: [1.         1.0019531  0.9992676  ... 0.99907124 0.9991518  1.0003937 ]\n",
      "Layer 8, Head 21: [1.         0.9987793  0.99871826 ... 1.0005529  0.99988806 1.0021344 ]\n",
      "Layer 8, Head 22: [1.        1.0009766 0.99823   ... 0.9993153 1.0004181 0.9979818]\n",
      "Layer 8, Head 23: [1.         1.         1.0021973  ... 0.99945754 1.000308   1.0011346 ]\n",
      "Layer 8, Head 24: [1.         0.999527   0.99868774 ... 0.9988075  0.9989761  1.000238  ]\n",
      "Layer 8, Head 25: [1.        0.9998627 1.0004272 ... 1.0007722 0.9986267 1.0016162]\n",
      "Layer 8, Head 26: [1.         0.99871826 0.9992676  ... 0.9993522  0.9985212  0.9989503 ]\n",
      "Layer 8, Head 27: [1.        1.0006104 1.001709  ... 1.000427  1.0003906 1.0003315]\n",
      "Layer 8, Head 28: [1.         1.0007935  0.99868774 ... 1.000446   1.0005035  0.99993   ]\n",
      "Layer 8, Head 29: [1.         1.0012817  0.9998169  ... 0.99957705 1.000548   0.9997705 ]\n",
      "Layer 8, Head 30: [1.         1.0019531  0.99902344 ... 1.0004919  0.99932647 0.99929136]\n",
      "Layer 8, Head 31: [1.         0.998291   0.99871826 ... 0.999887   0.99982816 0.99994886]\n",
      "Layer 9, Head 0: [1.         1.0014648  0.99902344 ... 1.0000858  0.99993044 1.0012478 ]\n",
      "Layer 9, Head 1: [1.         1.0006104  0.99853516 ... 1.0018686  0.9991671  0.9984408 ]\n",
      "Layer 9, Head 2: [1.         0.99853516 0.9985657  ... 1.0000432  0.99940455 0.99967885]\n",
      "Layer 9, Head 3: [1.         0.99890137 1.0009766  ... 0.9993058  1.0005552  0.99913585]\n",
      "Layer 9, Head 4: [1.         1.0008545  0.99890137 ... 1.0005218  0.99974906 1.000408  ]\n",
      "Layer 9, Head 5: [1.         1.0009766  1.0014648  ... 1.0010132  0.99949634 1.0000987 ]\n",
      "Layer 9, Head 6: [1.         0.99902344 1.0004883  ... 0.9992522  0.99838394 1.0004044 ]\n",
      "Layer 9, Head 7: [1.         0.99853516 1.001831   ... 0.9980893  0.99925524 1.001016  ]\n",
      "Layer 9, Head 8: [1.         0.99853516 0.9991455  ... 0.9991787  1.0006181  0.9996245 ]\n",
      "Layer 9, Head 9: [1.        1.0007324 1.0002441 ... 1.0004706 1.0010433 0.9991752]\n",
      "Layer 9, Head 10: [1.         1.         1.         ... 0.999175   1.0003119  0.99928784]\n",
      "Layer 9, Head 11: [1.         1.0019531  1.0004883  ... 1.0020201  0.9988404  0.99918604]\n",
      "Layer 9, Head 12: [1.         0.99887085 0.99848175 ... 0.99851596 0.99940026 1.0014162 ]\n",
      "Layer 9, Head 13: [1.         0.99816895 0.9996033  ... 1.0009038  0.9999542  1.0002338 ]\n",
      "Layer 9, Head 14: [1.         1.0010986  1.0000916  ... 1.0003173  0.99962336 0.99954367]\n",
      "Layer 9, Head 15: [1.         0.99998474 0.9987793  ... 1.0009329  1.0003608  1.0010705 ]\n",
      "Layer 9, Head 16: [1.         0.9992676  0.9984131  ... 0.999859   0.9986958  0.99867404]\n",
      "Layer 9, Head 17: [1.         0.99938965 0.9984436  ... 1.000236   1.0002257  0.99976265]\n",
      "Layer 9, Head 18: [1.        0.9980469 1.001709  ... 1.0004226 0.9989847 0.9992697]\n",
      "Layer 9, Head 19: [1.         1.0019531  0.9991455  ... 1.0000566  0.99814534 0.9983671 ]\n",
      "Layer 9, Head 20: [1.         1.0004883  1.0009766  ... 0.9996296  1.0003128  0.99884075]\n",
      "Layer 9, Head 21: [1.        0.9992676 1.0013428 ... 1.0008357 1.0011514 1.0004047]\n",
      "Layer 9, Head 22: [1.         1.00177    0.99990845 ... 1.001209   0.9998625  0.998462  ]\n",
      "Layer 9, Head 23: [1.        0.9990845 1.0003052 ... 0.9995482 0.9997945 0.9981529]\n",
      "Layer 9, Head 24: [1.        1.0012207 0.9995117 ... 0.9991138 1.0009997 0.9998974]\n",
      "Layer 9, Head 25: [1.         0.99890137 0.99823    ... 1.0015378  0.99835324 1.0008714 ]\n",
      "Layer 9, Head 26: [1.         1.0008545  0.99823    ... 0.9999312  1.0007907  0.99920535]\n",
      "Layer 9, Head 27: [1.         1.0012207  1.0002594  ... 0.9992368  0.99865067 0.9998188 ]\n",
      "Layer 9, Head 28: [1.         0.99902344 1.001709   ... 1.0004547  0.99955016 0.9979388 ]\n",
      "Layer 9, Head 29: [1.         0.9996338  0.9996948  ... 0.99928224 1.0006442  0.99881715]\n",
      "Layer 9, Head 30: [1.         1.0019531  0.9990845  ... 0.9992696  0.99894404 1.0018188 ]\n",
      "Layer 9, Head 31: [1.         1.0004883  0.99975586 ... 0.9998925  0.9987283  1.0002761 ]\n",
      "Layer 10, Head 0: [1.         1.         0.99902344 ... 1.0000584  1.0021033  0.9982643 ]\n",
      "Layer 10, Head 1: [1.         1.0012207  0.999115   ... 0.9996003  0.99966097 1.0003023 ]\n",
      "Layer 10, Head 2: [1.        0.9987793 1.0008698 ... 1.0001801 0.998832  1.0005987]\n",
      "Layer 10, Head 3: [1.        1.0019531 0.9992676 ... 1.0001937 0.9989003 1.0000799]\n",
      "Layer 10, Head 4: [1.         1.001709   0.9996948  ... 1.0011528  0.99994546 1.0008165 ]\n",
      "Layer 10, Head 5: [1.         0.9987793  0.9990082  ... 0.99952334 1.0008744  0.99828494]\n",
      "Layer 10, Head 6: [1.         0.9998779  0.99819946 ... 1.0001999  0.9990696  0.9990517 ]\n",
      "Layer 10, Head 7: [1.         1.0009766  1.0018005  ... 1.0017099  1.0018431  0.99833643]\n",
      "Layer 10, Head 8: [1.        1.0002441 1.0006104 ... 0.9993962 1.0002713 1.0007786]\n",
      "Layer 10, Head 9: [1.        0.9992676 1.0007324 ... 1.0003428 1.0005556 1.000922 ]\n",
      "Layer 10, Head 10: [1.         1.0009766  0.99975586 ... 1.0007857  1.00119    1.0002863 ]\n",
      "Layer 10, Head 11: [1.         1.0009766  0.99816895 ... 1.0015275  0.9992913  1.0009236 ]\n",
      "Layer 10, Head 12: [1.         0.99975586 1.0018616  ... 0.9987395  0.9989596  0.99805915]\n",
      "Layer 10, Head 13: [1.        1.0002441 1.0009003 ... 0.9993832 0.999463  1.0018034]\n",
      "Layer 10, Head 14: [1.         1.         0.998291   ... 0.99890167 0.99928844 0.9983312 ]\n",
      "Layer 10, Head 15: [1.         0.99853516 0.9984131  ... 1.0012255  1.0001231  0.99987125]\n",
      "Layer 10, Head 16: [1.         0.9987793  1.0006409  ... 0.9981512  0.99817926 0.99869573]\n",
      "Layer 10, Head 17: [1.         1.0019531  0.9992676  ... 0.99989915 1.0009252  0.99932075]\n",
      "Layer 10, Head 18: [1.         1.0009766  1.0007324  ... 0.99886525 1.0000281  0.99899364]\n",
      "Layer 10, Head 19: [1.        0.9992676 1.0012817 ... 1.0001726 1.0006573 1.0008065]\n",
      "Layer 10, Head 20: [1.         0.9996948  1.0009766  ... 0.99963194 1.0003419  0.99826515]\n",
      "Layer 10, Head 21: [1.         0.9998779  1.0001221  ... 0.99997747 1.0002464  1.0002512 ]\n",
      "Layer 10, Head 22: [1.        1.0009766 0.9989624 ... 0.998526  1.0000278 1.000087 ]\n",
      "Layer 10, Head 23: [1.         0.9986572  1.0001831  ... 1.0000303  1.0002234  0.99806064]\n",
      "Layer 10, Head 24: [1.        0.9995117 1.0015869 ... 1.0004938 1.0000837 1.000444 ]\n",
      "Layer 10, Head 25: [1.         0.99902344 0.9992981  ... 1.0020034  0.9989097  0.9991801 ]\n",
      "Layer 10, Head 26: [1.        0.9985962 1.000061  ... 1.0008928 1.0008016 1.0006924]\n",
      "Layer 10, Head 27: [1.         0.99938965 1.0016174  ... 0.9986918  0.9980755  0.9991995 ]\n",
      "Layer 10, Head 28: [1.         1.0014648  0.9986572  ... 0.99977744 1.0005938  0.99862564]\n",
      "Layer 10, Head 29: [1.         0.99902344 0.9984131  ... 1.0000851  0.9987173  0.999158  ]\n",
      "Layer 10, Head 30: [1.         0.99853516 1.001709   ... 0.9982571  0.9995396  1.0015104 ]\n",
      "Layer 10, Head 31: [1.         0.9995117  0.99902344 ... 1.0006502  1.0003561  0.9987564 ]\n",
      "Layer 11, Head 0: [1.         1.0015869  0.9994812  ... 1.0012339  0.99836177 1.0015386 ]\n",
      "Layer 11, Head 1: [1.         1.         0.9996948  ... 1.000706   0.99955094 0.99978054]\n",
      "Layer 11, Head 2: [1.        0.9984131 1.0004883 ... 1.0013361 0.9991571 1.0003214]\n",
      "Layer 11, Head 3: [1.         1.0019531  0.99850464 ... 1.001561   1.0017004  1.0008352 ]\n",
      "Layer 11, Head 4: [1.         1.0004883  0.9993286  ... 0.99936503 1.0014862  1.0000309 ]\n",
      "Layer 11, Head 5: [1.        1.        0.9992218 ... 1.001851  1.0001386 0.9989086]\n",
      "Layer 11, Head 6: [1.         1.0014648  0.9995117  ... 0.99974227 1.0006104  0.9995674 ]\n",
      "Layer 11, Head 7: [1.         0.99902344 1.0001221  ... 1.0000087  0.99963623 0.99996424]\n",
      "Layer 11, Head 8: [1.         0.9980469  1.0007935  ... 0.99978566 1.0005971  0.9997071 ]\n",
      "Layer 11, Head 9: [1.         0.99938965 0.9992981  ... 0.99949676 0.99965596 1.0013765 ]\n",
      "Layer 11, Head 10: [1.         0.99902344 1.001709   ... 1.0009887  0.99962425 1.0013165 ]\n",
      "Layer 11, Head 11: [1.         0.9998779  0.99887085 ... 1.0002983  1.0002834  0.9999207 ]\n",
      "Layer 11, Head 12: [1.         1.0007324  0.99853516 ... 0.99980414 1.0006312  1.0015281 ]\n",
      "Layer 11, Head 13: [1.         0.998291   0.9998398  ... 0.99970376 0.9994395  0.9999466 ]\n",
      "Layer 11, Head 14: [1.        1.0002441 0.9986572 ... 0.9998261 1.0009491 0.9995523]\n",
      "Layer 11, Head 15: [1.         0.99975586 1.001709   ... 0.99974275 0.9996372  0.99855435]\n",
      "Layer 11, Head 16: [1.         0.99853516 1.001709   ... 1.0003223  0.99952996 0.9995719 ]\n",
      "Layer 11, Head 17: [1.        0.9987793 1.001831  ... 1.001194  0.9993581 1.0007114]\n",
      "Layer 11, Head 18: [1.         1.0009766  0.9993286  ... 1.0003392  0.9993302  0.99950314]\n",
      "Layer 11, Head 19: [1.         1.0014648  1.0009766  ... 0.9997771  0.99986356 0.99890375]\n",
      "Layer 11, Head 20: [1.         0.99853516 0.99871063 ... 1.0008906  0.9992813  1.0010965 ]\n",
      "Layer 11, Head 21: [1.        1.        1.0017395 ... 1.0001711 1.0018059 1.0016592]\n",
      "Layer 11, Head 22: [1.        0.9980469 1.0015869 ... 0.9996421 1.0008502 1.0019293]\n",
      "Layer 11, Head 23: [1.         1.0019531  1.0009766  ... 0.99887145 0.999087   0.9989816 ]\n",
      "Layer 11, Head 24: [1.         1.0014648  1.0015869  ... 0.99889296 0.99874043 1.00035   ]\n",
      "Layer 11, Head 25: [1.         0.9991455  1.0001221  ... 1.0000525  1.0005631  0.99978024]\n",
      "Layer 11, Head 26: [1.         1.0007324  0.9998779  ... 1.000239   0.99859893 0.99996924]\n",
      "Layer 11, Head 27: [1.        0.9986572 1.0012665 ... 1.0000381 1.0003047 1.0017638]\n",
      "Layer 11, Head 28: [1.        0.9980469 1.0010071 ... 1.0018405 0.9984057 1.0017639]\n",
      "Layer 11, Head 29: [1.         1.0003662  1.0018921  ... 1.0005255  0.9996222  0.99957514]\n",
      "Layer 11, Head 30: [1.        1.0012207 1.0017395 ... 1.0002339 0.9996601 1.0000057]\n",
      "Layer 11, Head 31: [1.         0.99975586 0.99823    ... 0.9996704  0.9997785  0.99976975]\n",
      "Layer 12, Head 0: [1.        1.001709  1.0004883 ... 0.997947  1.0002811 1.0005984]\n",
      "Layer 12, Head 1: [1.         1.00177    1.0013123  ... 1.001669   1.0000253  0.99927163]\n",
      "Layer 12, Head 2: [1.         1.0004883  0.99975586 ... 0.99975383 0.99943113 1.0001945 ]\n",
      "Layer 12, Head 3: [1.         1.         0.99853516 ... 0.99945736 0.9995563  1.0009692 ]\n",
      "Layer 12, Head 4: [1.        0.9987793 1.0007324 ... 0.9993712 1.0007371 0.9999437]\n",
      "Layer 12, Head 5: [1.         0.99902344 1.0001831  ... 0.99909467 1.0007517  1.001749  ]\n",
      "Layer 12, Head 6: [1.         1.0019531  0.9984741  ... 0.9993508  0.99894464 0.9993855 ]\n",
      "Layer 12, Head 7: [1.         0.99853516 1.0007935  ... 1.0011733  1.000325   1.0007501 ]\n",
      "Layer 12, Head 8: [1.        1.0009766 0.9996033 ... 1.0002599 1.0008459 1.0000463]\n",
      "Layer 12, Head 9: [1.         1.0004883  1.0007324  ... 0.99981785 1.001014   0.99942577]\n",
      "Layer 12, Head 10: [1.         1.0004883  1.0007935  ... 1.0003496  0.99941283 0.9994453 ]\n",
      "Layer 12, Head 11: [1.         0.99853516 0.9981079  ... 0.99936664 1.0013573  0.9982091 ]\n",
      "Layer 12, Head 12: [1.         0.9980469  1.0014648  ... 1.0002202  0.99898434 1.0002913 ]\n",
      "Layer 12, Head 13: [1.         1.0014648  1.0003662  ... 1.0003564  1.0000354  0.99881136]\n",
      "Layer 12, Head 14: [1.        1.0014648 0.9992676 ... 0.999743  0.9996427 1.0004113]\n",
      "Layer 12, Head 15: [1.         1.0004883  0.998291   ... 0.9987922  1.0003816  0.99961793]\n",
      "Layer 12, Head 16: [1.         0.9992676  1.0001221  ... 0.99990994 0.9995791  0.999701  ]\n",
      "Layer 12, Head 17: [1.         0.9991455  1.0003662  ... 0.99977016 0.9992224  0.9989171 ]\n",
      "Layer 12, Head 18: [1.        0.9987793 1.0013428 ... 0.9993454 1.0002432 1.0013452]\n",
      "Layer 12, Head 19: [1.        0.9995117 0.9994507 ... 1.0004442 1.0010748 1.0005343]\n",
      "Layer 12, Head 20: [1.        0.9980469 1.0009766 ... 1.0004774 0.9992745 1.0000788]\n",
      "Layer 12, Head 21: [1.         0.99816895 0.9991455  ... 0.99914145 0.9998061  1.0000063 ]\n",
      "Layer 12, Head 22: [1.         0.9992676  1.0003662  ... 0.99834144 1.0002685  1.0008333 ]\n",
      "Layer 12, Head 23: [1.        1.0009766 0.9979248 ... 1.0015372 1.001202  0.9996689]\n",
      "Layer 12, Head 24: [1.        1.0019531 0.9994507 ... 0.9992409 0.9995007 0.9988646]\n",
      "Layer 12, Head 25: [1.        1.0004883 1.0015869 ... 1.0004971 0.9978459 0.9985811]\n",
      "Layer 12, Head 26: [1.         0.99902344 0.9998779  ... 0.9991501  0.9992721  0.99803376]\n",
      "Layer 12, Head 27: [1.        1.0009766 1.001709  ... 1.0003818 1.0017498 1.000178 ]\n",
      "Layer 12, Head 28: [1.        1.0004883 0.9986572 ... 1.0004818 1.0018166 1.0014719]\n",
      "Layer 12, Head 29: [1.         0.99902344 0.998291   ... 1.0002929  0.9997213  0.99979186]\n",
      "Layer 12, Head 30: [1.        1.0019531 1.001709  ... 0.9992728 1.0007205 0.9985694]\n",
      "Layer 12, Head 31: [1.         1.0019531  0.99975586 ... 0.99821514 1.0000818  1.0004631 ]\n",
      "Layer 13, Head 0: [1.         0.99853516 1.0004883  ... 0.9996834  1.0000606  1.0001994 ]\n",
      "Layer 13, Head 1: [1.         1.0002441  0.99890137 ... 1.0003119  0.99963623 0.99986964]\n",
      "Layer 13, Head 2: [1.         1.0014648  0.99853516 ... 0.99958295 0.99955416 0.99891675]\n",
      "Layer 13, Head 3: [1.         1.0012207  1.0007324  ... 0.99922895 0.9998815  1.0001762 ]\n",
      "Layer 13, Head 4: [1.         0.9996338  0.99990845 ... 0.99920386 1.0001645  0.99985075]\n",
      "Layer 13, Head 5: [1.         1.0019531  1.0007019  ... 0.99918485 1.0006106  1.0014535 ]\n",
      "Layer 13, Head 6: [1.         0.9985962  0.9988861  ... 1.0007981  0.99962187 0.99921244]\n",
      "Layer 13, Head 7: [1.         1.0001221  1.0007935  ... 1.0011425  1.0004752  0.99925697]\n",
      "Layer 13, Head 8: [1.         1.0004883  1.00177    ... 0.99950206 1.0010526  0.99890214]\n",
      "Layer 13, Head 9: [1.         1.         0.9985962  ... 0.99823785 0.99981344 1.0015413 ]\n",
      "Layer 13, Head 10: [1.        1.0009766 1.00177   ... 1.0006304 0.9991754 0.9989138]\n",
      "Layer 13, Head 11: [1.         0.9996338  1.0018616  ... 1.0012338  0.99984133 1.0002908 ]\n",
      "Layer 13, Head 12: [1.         0.99975586 1.0016632  ... 0.9985768  1.0015285  1.0017978 ]\n",
      "Layer 13, Head 13: [1.         1.0006104  1.0006104  ... 1.00028    0.99966705 1.0006964 ]\n",
      "Layer 13, Head 14: [1.         1.0009766  0.9990845  ... 1.0003097  0.99961925 0.9989077 ]\n",
      "Layer 13, Head 15: [1.        1.0010986 1.0008545 ... 0.9994331 1.0008266 1.0000539]\n",
      "Layer 13, Head 16: [1.         1.0014648  0.9984741  ... 0.99924624 0.99989563 0.9995543 ]\n",
      "Layer 13, Head 17: [1.         1.001709   0.99931335 ... 1.0008622  0.9990726  1.0000136 ]\n",
      "Layer 13, Head 18: [1.         0.998291   0.9984131  ... 1.0002614  0.99952453 1.0008378 ]\n",
      "Layer 13, Head 19: [1.         0.99902344 0.9984741  ... 1.000796   1.0008041  0.99946976]\n",
      "Layer 13, Head 20: [1.         1.001709   1.00177    ... 0.99814343 0.9999922  1.0008113 ]\n",
      "Layer 13, Head 21: [1.        0.9994812 0.99823   ... 0.999485  0.9998858 1.0003881]\n",
      "Layer 13, Head 22: [1.         0.9994507  0.9988098  ... 0.99962306 0.99884546 0.9979079 ]\n",
      "Layer 13, Head 23: [1.         1.0007324  1.0003052  ... 1.0001447  0.99963206 0.99889493]\n",
      "Layer 13, Head 24: [1.         1.0014648  0.99853516 ... 0.9999428  1.0005928  0.9996476 ]\n",
      "Layer 13, Head 25: [1.         1.         0.99938965 ... 0.99932665 1.0012637  1.0004425 ]\n",
      "Layer 13, Head 26: [1.         1.0009766  0.9985657  ... 0.9983863  0.99943036 1.0007837 ]\n",
      "Layer 13, Head 27: [1.         1.         1.0004883  ... 1.0000453  0.99915147 1.0007038 ]\n",
      "Layer 13, Head 28: [1.         1.         0.99902344 ... 0.9999501  1.0010012  1.00066   ]\n",
      "Layer 13, Head 29: [1.         0.99853516 1.0009766  ... 0.9998703  0.9994403  1.0000149 ]\n",
      "Layer 13, Head 30: [1.        1.0002441 1.0014038 ... 0.9994371 1.0002652 1.000376 ]\n",
      "Layer 13, Head 31: [1.        1.        0.9992676 ... 0.9999869 0.9997202 0.9991071]\n",
      "Layer 14, Head 0: [1.         1.0012817  0.9996948  ... 1.0019237  1.0015877  0.99820274]\n",
      "Layer 14, Head 1: [1.         0.99993896 0.9985657  ... 0.9989891  1.000334   0.9992148 ]\n",
      "Layer 14, Head 2: [1.         1.0009155  0.99949646 ... 0.99905354 1.000475   0.9994458 ]\n",
      "Layer 14, Head 3: [1.         0.99957275 0.9998474  ... 1.0006735  1.0004615  0.99991673]\n",
      "Layer 14, Head 4: [1.         0.998291   0.9998779  ... 0.99936545 0.9996765  0.99981236]\n",
      "Layer 14, Head 5: [1.         0.99920654 0.998291   ... 0.999737   0.99983454 0.9997408 ]\n",
      "Layer 14, Head 6: [1.         1.0012207  0.9994812  ... 0.99883974 0.9998221  0.9998354 ]\n",
      "Layer 14, Head 7: [1.         1.0002441  0.99816895 ... 0.9997897  1.0000929  1.0006583 ]\n",
      "Layer 14, Head 8: [1.         0.99853516 1.         ... 0.9995141  0.9996565  0.9996438 ]\n",
      "Layer 14, Head 9: [1.        0.9995117 1.0002441 ... 0.9997498 1.0003519 1.0001059]\n",
      "Layer 14, Head 10: [1.        0.9987793 1.0008545 ... 1.0006466 1.0009624 1.0007677]\n",
      "Layer 14, Head 11: [1.        1.0004883 1.        ... 1.001632  1.0004953 1.0003668]\n",
      "Layer 14, Head 12: [1.         1.0015869  0.9986572  ... 0.99973524 0.99893117 1.000342  ]\n",
      "Layer 14, Head 13: [1.         1.0003662  0.9987793  ... 0.99972606 0.99990904 1.0003397 ]\n",
      "Layer 14, Head 14: [1.         1.         1.0004883  ... 0.99987286 0.9999745  1.0003492 ]\n",
      "Layer 14, Head 15: [1.        0.998291  1.0011597 ... 1.0016339 1.0011648 1.0004431]\n",
      "Layer 14, Head 16: [1.         1.0019531  0.99975586 ... 1.0005797  0.9996757  0.99994177]\n",
      "Layer 14, Head 17: [1.        1.0009766 0.9980469 ... 0.9996648 0.9988867 1.0006578]\n",
      "Layer 14, Head 18: [1.        1.0004883 0.9993286 ... 0.9990772 0.9983687 1.0010988]\n",
      "Layer 14, Head 19: [1.         1.0009766  1.0011215  ... 1.000072   0.99863094 1.0004156 ]\n",
      "Layer 14, Head 20: [1.         1.0003052  0.99853516 ... 1.0003457  0.99969375 0.9994117 ]\n",
      "Layer 14, Head 21: [1.        1.0007324 1.0003662 ... 0.999043  1.0005621 0.9998985]\n",
      "Layer 14, Head 22: [1.        1.0010986 0.998703  ... 0.9992206 1.0008342 1.0000744]\n",
      "Layer 14, Head 23: [1.        1.0002441 1.0015259 ... 0.9989215 1.0001589 1.0010973]\n",
      "Layer 14, Head 24: [1.         1.0003662  0.99923706 ... 0.99836063 0.9991635  0.99988014]\n",
      "Layer 14, Head 25: [1.         1.0001221  1.0015564  ... 0.99833494 1.0000986  1.0016859 ]\n",
      "Layer 14, Head 26: [1.         1.0009766  1.0019531  ... 1.0001872  0.99962294 0.9995596 ]\n",
      "Layer 14, Head 27: [1.         0.99853516 0.99938965 ... 1.0004936  0.9985931  0.9986638 ]\n",
      "Layer 14, Head 28: [1.         1.0019531  0.998291   ... 1.000337   0.9998392  0.99993014]\n",
      "Layer 14, Head 29: [1.        1.0014648 1.0007324 ... 0.9999658 1.0003449 1.0002799]\n",
      "Layer 14, Head 30: [1.         0.998291   0.99923706 ... 1.0000198  0.999905   1.0004003 ]\n",
      "Layer 14, Head 31: [1.         0.9998169  0.99853516 ... 1.0004989  0.99966514 1.0000316 ]\n",
      "Layer 15, Head 0: [1.         1.0004883  0.9992676  ... 0.99941415 0.99931467 1.0005646 ]\n",
      "Layer 15, Head 1: [1.        0.99823   1.0015869 ... 1.0015396 1.0019158 1.0019075]\n",
      "Layer 15, Head 2: [1.        1.0007324 1.0015259 ... 1.0011545 1.0010343 1.0017045]\n",
      "Layer 15, Head 3: [1.         1.0001221  0.9987793  ... 0.99953705 1.0006287  0.9991143 ]\n",
      "Layer 15, Head 4: [1.        1.0004883 1.0004883 ... 0.9995539 0.9999581 1.0002357]\n",
      "Layer 15, Head 5: [1.         1.0015869  0.99853516 ... 1.0000552  0.9985254  0.9992428 ]\n",
      "Layer 15, Head 6: [1.        0.9980469 1.0019531 ... 0.9992075 1.0001485 0.9999464]\n",
      "Layer 15, Head 7: [1.         0.99902344 1.0004883  ... 0.9985753  0.99975765 1.0004119 ]\n",
      "Layer 15, Head 8: [1.         0.99902344 0.9987793  ... 0.99872303 1.0009124  0.99817467]\n",
      "Layer 15, Head 9: [1.         0.99853516 1.0004883  ... 1.0011959  1.0016912  0.99920654]\n",
      "Layer 15, Head 10: [1.        1.        1.        ... 1.0001204 1.0004964 1.0005784]\n",
      "Layer 15, Head 11: [1.         1.001709   0.99902344 ... 1.000508   0.99992806 1.0008957 ]\n",
      "Layer 15, Head 12: [1.         0.99902344 1.0007324  ... 1.0000857  1.0021154  0.99923825]\n",
      "Layer 15, Head 13: [1.         1.0004883  1.0001221  ... 1.0014272  0.99808806 0.99851704]\n",
      "Layer 15, Head 14: [1.         0.99816895 0.9995117  ... 0.9985386  1.0004933  1.0007383 ]\n",
      "Layer 15, Head 15: [1.         1.0015869  0.9996338  ... 0.99941087 1.0000985  1.0006952 ]\n",
      "Layer 15, Head 16: [1.        0.9995117 0.9987793 ... 0.9997903 1.0010463 1.0009527]\n",
      "Layer 15, Head 17: [1.         0.99890137 0.998291   ... 1.0012027  0.99918735 0.9993818 ]\n",
      "Layer 15, Head 18: [1.         0.99902344 0.99938965 ... 0.9994768  0.99991703 0.9996705 ]\n",
      "Layer 15, Head 19: [1.         1.0009766  1.0002441  ... 1.0006914  0.99820006 1.0011046 ]\n",
      "Layer 15, Head 20: [1.         1.0015869  0.99853516 ... 1.0003891  1.0004905  0.9997467 ]\n",
      "Layer 15, Head 21: [1.         0.9996338  0.99923706 ... 0.9990587  1.0007782  0.9998538 ]\n",
      "Layer 15, Head 22: [1.         0.9987793  1.0014038  ... 0.9989642  0.99863756 0.9980956 ]\n",
      "Layer 15, Head 23: [1.         1.0012207  0.9981079  ... 0.99920607 1.0000883  0.9997632 ]\n",
      "Layer 15, Head 24: [1.        1.0009766 0.9995117 ... 1.0011892 1.0002006 0.999015 ]\n",
      "Layer 15, Head 25: [1.         0.99823    0.998291   ... 0.99912685 1.000505   0.9989636 ]\n",
      "Layer 15, Head 26: [1.        1.        1.0006714 ... 0.9997772 1.001623  1.0002687]\n",
      "Layer 15, Head 27: [1.         1.0015869  0.9986572  ... 1.0002863  0.99908984 0.9981165 ]\n",
      "Layer 15, Head 28: [1.        1.0006714 0.9989166 ... 0.9982318 1.0004873 1.0017935]\n",
      "Layer 15, Head 29: [1.         1.         0.99993896 ... 1.0000676  0.99956226 0.99886155]\n",
      "Layer 15, Head 30: [1.         0.99816895 0.9984436  ... 0.99907017 0.99950385 1.0009025 ]\n",
      "Layer 15, Head 31: [1.         1.0019531  0.99902344 ... 0.99915206 0.99987173 0.9992565 ]\n",
      "Layer 16, Head 0: [1.        0.9996948 1.001831  ... 0.9994503 1.0002189 1.0002635]\n",
      "Layer 16, Head 1: [1.         0.99835205 0.9991455  ... 1.0006261  1.0007489  0.9993159 ]\n",
      "Layer 16, Head 2: [1.        0.9984131 1.0008545 ... 1.0001444 1.001102  1.0004752]\n",
      "Layer 16, Head 3: [1.        1.0002441 0.999115  ... 0.9987955 1.0014148 1.0006852]\n",
      "Layer 16, Head 4: [1.         1.0009155  0.99902344 ... 1.000015   1.0007085  0.9993122 ]\n",
      "Layer 16, Head 5: [1.        1.0002441 1.0019531 ... 0.9998639 1.0008645 1.0003843]\n",
      "Layer 16, Head 6: [1.         0.9985962  0.9998779  ... 0.99801815 1.001282   0.99873245]\n",
      "Layer 16, Head 7: [1.        1.0013428 1.0013428 ... 1.001396  0.9989754 1.0008954]\n",
      "Layer 16, Head 8: [1.         1.0004883  0.99902344 ... 1.0000582  1.000884   0.99925256]\n",
      "Layer 16, Head 9: [1.         1.0018921  0.99954224 ... 1.0004554  0.9980056  1.0008682 ]\n",
      "Layer 16, Head 10: [1.         0.99819946 1.0005798  ... 1.0007772  0.99920756 1.0004815 ]\n",
      "Layer 16, Head 11: [1.         0.99823    0.99832153 ... 1.0006335  1.000803   1.000876  ]\n",
      "Layer 16, Head 12: [1.        1.0004883 1.0012207 ... 1.000879  1.0003933 1.0002794]\n",
      "Layer 16, Head 13: [1.        1.0014648 1.001831  ... 1.0005598 1.000031  1.0001447]\n",
      "Layer 16, Head 14: [1.         0.99816895 1.0012817  ... 0.9988669  0.99984074 0.9998038 ]\n",
      "Layer 16, Head 15: [1.         0.9995117  0.9992676  ... 0.9989922  0.9994525  0.99975955]\n",
      "Layer 16, Head 16: [1.         0.9980469  0.9995117  ... 0.99932075 1.0000169  1.0003065 ]\n",
      "Layer 16, Head 17: [1.         1.0002441  0.9992676  ... 0.99921346 0.9988485  1.0014199 ]\n",
      "Layer 16, Head 18: [1.         1.0008545  1.001709   ... 1.000083   0.99851406 0.99932814]\n",
      "Layer 16, Head 19: [1.         0.99975586 1.0006104  ... 0.9981456  0.9986559  1.0008976 ]\n",
      "Layer 16, Head 20: [1.         0.9990845  1.0000916  ... 0.99876106 0.9987768  0.9994387 ]\n",
      "Layer 16, Head 21: [1.         1.0008545  0.99832153 ... 1.0001286  1.0005084  1.0009243 ]\n",
      "Layer 16, Head 22: [1.         0.99902344 1.0009766  ... 1.0004593  1.0008464  1.0005065 ]\n",
      "Layer 16, Head 23: [1.         0.99920654 0.9990845  ... 0.9990344  1.0006917  0.9984567 ]\n",
      "Layer 16, Head 24: [1.        1.0019531 1.0018921 ... 0.9986745 1.000224  1.0016772]\n",
      "Layer 16, Head 25: [1.         1.0007324  0.9980469  ... 0.9988688  0.9986427  0.99808264]\n",
      "Layer 16, Head 26: [1.         1.0001221  0.99938965 ... 0.99909526 1.0009214  0.9989213 ]\n",
      "Layer 16, Head 27: [1.         1.0010376  0.9987488  ... 0.99962234 1.0014385  1.0016055 ]\n",
      "Layer 16, Head 28: [1.         1.0009766  1.0014648  ... 1.0017493  0.9982977  0.99980485]\n",
      "Layer 16, Head 29: [1.         1.0019531  0.9995117  ... 1.001971   1.0003517  0.99975604]\n",
      "Layer 16, Head 30: [1.        0.9980469 0.9980469 ... 0.9993286 1.0007509 1.0012681]\n",
      "Layer 16, Head 31: [1.         1.0004883  0.99853516 ... 0.99931175 1.0008886  1.0015124 ]\n",
      "Layer 17, Head 0: [1.        1.00177   1.0001221 ... 1.0004172 1.0005252 1.0008582]\n",
      "Layer 17, Head 1: [1.         0.99853516 1.0008545  ... 0.99883246 1.001545   1.0018786 ]\n",
      "Layer 17, Head 2: [1.         1.0003662  0.99902344 ... 1.0015342  1.0002148  1.000756  ]\n",
      "Layer 17, Head 3: [1.        1.0015259 1.0010986 ... 1.0016636 1.0019317 0.999223 ]\n",
      "Layer 17, Head 4: [1.         0.9996338  1.0004883  ... 0.9982978  0.99826384 1.0010948 ]\n",
      "Layer 17, Head 5: [1.         1.0019531  1.0014648  ... 1.0018003  0.9997535  0.99908817]\n",
      "Layer 17, Head 6: [1.        0.9980469 0.9986572 ... 1.0004652 1.0014087 1.0002953]\n",
      "Layer 17, Head 7: [1.         1.0004883  1.0013428  ... 0.99850005 0.999714   1.0014224 ]\n",
      "Layer 17, Head 8: [1.        0.9993286 0.9990845 ... 1.0006745 0.9982935 1.0016906]\n",
      "Layer 17, Head 9: [1.         1.0004883  0.9980469  ... 0.99944615 0.9999999  1.0011216 ]\n",
      "Layer 17, Head 10: [1.        1.0009766 1.0004883 ... 1.0004756 0.9993648 0.9982791]\n",
      "Layer 17, Head 11: [1.         1.0009766  0.99890137 ... 0.9992252  1.0001366  0.99941325]\n",
      "Layer 17, Head 12: [1.         0.99902344 0.99816895 ... 0.9998896  0.9995184  1.0006325 ]\n",
      "Layer 17, Head 13: [1.        1.        1.0015869 ... 1.0017072 1.0003163 1.0006366]\n",
      "Layer 17, Head 14: [1.         1.001709   1.0014648  ... 0.9992366  0.9994525  0.99943626]\n",
      "Layer 17, Head 15: [1.        1.0009766 1.0004883 ... 0.9989669 0.9994992 1.0002518]\n",
      "Layer 17, Head 16: [1.        0.9994507 1.0004883 ... 0.9990901 0.9999558 1.0003433]\n",
      "Layer 17, Head 17: [1.         1.0018692  0.9994354  ... 1.0001528  0.99849474 1.0016624 ]\n",
      "Layer 17, Head 18: [1.         0.99938965 0.999176   ... 0.9991395  1.0011812  0.99991727]\n",
      "Layer 17, Head 19: [1.         0.99823    1.0015869  ... 1.0019048  1.000523   0.99971867]\n",
      "Layer 17, Head 20: [1.         1.0010986  0.99890137 ... 1.0009562  0.99835217 0.99859077]\n",
      "Layer 17, Head 21: [1.         0.99957275 0.99988556 ... 0.99893796 1.0007555  1.0005167 ]\n",
      "Layer 17, Head 22: [1.         1.0019531  0.99938965 ... 0.99993813 0.99949014 0.9991919 ]\n",
      "Layer 17, Head 23: [1.         1.0010986  0.99990845 ... 0.9983671  0.9994556  1.0004377 ]\n",
      "Layer 17, Head 24: [1.         1.001709   0.99835205 ... 0.9993363  1.0009818  1.000828  ]\n",
      "Layer 17, Head 25: [1.         0.99975586 0.99920654 ... 1.0002227  1.0003903  0.9996015 ]\n",
      "Layer 17, Head 26: [1.        0.9987793 1.0004272 ... 1.0007026 0.9983085 1.0011945]\n",
      "Layer 17, Head 27: [1.        0.9995117 1.0003052 ... 0.9999692 1.0004363 0.9991475]\n",
      "Layer 17, Head 28: [1.         0.99853516 0.99938965 ... 1.0017947  1.0007049  1.0013089 ]\n",
      "Layer 17, Head 29: [1.         1.0000916  0.99842834 ... 0.9999149  0.9983324  1.0018647 ]\n",
      "Layer 17, Head 30: [1.        1.0007935 1.0002441 ... 0.998544  0.9994617 1.0003362]\n",
      "Layer 17, Head 31: [1.         0.99853516 1.0007782  ... 1.0002041  0.9984705  1.0001996 ]\n",
      "Layer 18, Head 0: [1.         0.99993896 0.9982605  ... 1.000535   0.9988159  1.0002842 ]\n",
      "Layer 18, Head 1: [1.         0.99853516 0.9992676  ... 1.000749   1.0005119  0.9995509 ]\n",
      "Layer 18, Head 2: [1.         0.9984131  0.9994507  ... 0.99833196 1.0010307  0.99963605]\n",
      "Layer 18, Head 3: [1.         0.99902344 1.0006104  ... 0.9998042  1.0005009  1.0016546 ]\n",
      "Layer 18, Head 4: [1.        1.0004883 0.9983139 ... 1.0018972 0.999812  0.9994739]\n",
      "Layer 18, Head 5: [1.         1.0018921  1.0008545  ... 0.9990793  0.99811006 1.0007474 ]\n",
      "Layer 18, Head 6: [1.         1.000061   1.         ... 0.9989956  1.0015552  0.99869317]\n",
      "Layer 18, Head 7: [1.         1.0004883  0.9995117  ... 0.99947804 1.0006144  0.9999159 ]\n",
      "Layer 18, Head 8: [1.         0.9987793  0.99853516 ... 0.9982567  0.998198   0.9985249 ]\n",
      "Layer 18, Head 9: [1.         0.9996338  1.0004272  ... 1.0010103  1.001786   0.99903435]\n",
      "Layer 18, Head 10: [1.         1.001709   0.99890137 ... 1.0009996  1.0003275  1.0008379 ]\n",
      "Layer 18, Head 11: [1.         0.9990845  0.99890137 ... 1.0004348  1.000038   1.0017122 ]\n",
      "Layer 18, Head 12: [1.         0.99902344 1.0014648  ... 0.99907374 0.99930143 1.0001135 ]\n",
      "Layer 18, Head 13: [1.         1.         0.99902344 ... 1.0000914  0.99939215 1.0017853 ]\n",
      "Layer 18, Head 14: [1.        0.9994507 1.000061  ... 0.9985723 1.0001163 0.9992858]\n",
      "Layer 18, Head 15: [1.         0.99975586 1.001709   ... 1.0000757  0.9984968  0.9985335 ]\n",
      "Layer 18, Head 16: [1.        0.9987793 0.9997864 ... 1.0014336 0.9999579 0.9983553]\n",
      "Layer 18, Head 17: [1.         0.9986572  1.0013428  ... 0.99985385 0.99949723 1.0003507 ]\n",
      "Layer 18, Head 18: [1.        1.0019531 1.0007019 ... 0.9994301 0.9995927 1.0008796]\n",
      "Layer 18, Head 19: [1.         0.99938965 1.0006104  ... 0.9992013  0.9986554  1.000796  ]\n",
      "Layer 18, Head 20: [1.        0.9993439 0.9989929 ... 0.9987399 1.0005162 1.0003915]\n",
      "Layer 18, Head 21: [1.         1.0014038  0.998497   ... 1.0003686  0.9988579  0.99854726]\n",
      "Layer 18, Head 22: [1.         0.99890137 1.0015259  ... 0.9993439  1.0000682  0.9996057 ]\n",
      "Layer 18, Head 23: [1.         0.99853516 1.0008545  ... 1.0014389  1.0015138  0.9995173 ]\n",
      "Layer 18, Head 24: [1.         0.99853516 1.0014648  ... 0.9985285  1.0014285  1.0004776 ]\n",
      "Layer 18, Head 25: [1.        1.0002441 1.0013428 ... 0.9984966 0.9988429 1.0003977]\n",
      "Layer 18, Head 26: [1.         0.9987793  0.99816895 ... 1.0005031  1.001929   0.99912953]\n",
      "Layer 18, Head 27: [1.        1.0013428 0.9996338 ... 1.000252  1.0017004 1.000101 ]\n",
      "Layer 18, Head 28: [1.         1.0015869  0.99954224 ... 1.0003763  0.99922013 1.0017865 ]\n",
      "Layer 18, Head 29: [1.        1.0015869 1.0015869 ... 0.9983444 1.001607  0.9992608]\n",
      "Layer 18, Head 30: [1.         0.9995117  1.0009918  ... 1.0010427  1.0001099  0.99924994]\n",
      "Layer 18, Head 31: [1.         1.0012817  0.99871826 ... 1.0008969  1.0016733  1.0014788 ]\n",
      "Layer 19, Head 0: [1.        1.0000305 0.9981384 ... 1.0016522 1.0007702 1.0013981]\n",
      "Layer 19, Head 1: [1.         0.99942017 1.0004272  ... 1.0008093  0.99979585 1.0011233 ]\n",
      "Layer 19, Head 2: [1.        1.0004883 1.0017242 ... 1.0016108 1.0017694 0.9983528]\n",
      "Layer 19, Head 3: [1.         1.0019531  1.0014496  ... 1.0009758  1.001441   0.99977124]\n",
      "Layer 19, Head 4: [1.         0.99853516 0.9996338  ... 1.0019368  0.9984458  1.0008036 ]\n",
      "Layer 19, Head 5: [1.        1.0003052 0.9998169 ... 1.0001502 1.0003262 1.001435 ]\n",
      "Layer 19, Head 6: [1.         0.9998169  0.99876404 ... 0.999454   0.9984841  1.0002928 ]\n",
      "Layer 19, Head 7: [1.        1.0002441 0.9998474 ... 1.0016956 1.0003442 0.9982304]\n",
      "Layer 19, Head 8: [1.         0.99902344 0.9991455  ... 0.999398   1.0013742  1.0016489 ]\n",
      "Layer 19, Head 9: [1.         0.9984131  0.99850464 ... 0.9989172  1.0019484  1.0004218 ]\n",
      "Layer 19, Head 10: [1.         1.0002441  1.0004883  ... 0.99967945 0.99869645 1.0012851 ]\n",
      "Layer 19, Head 11: [1.        1.        1.0015869 ... 1.0015678 1.0003926 0.9983965]\n",
      "Layer 19, Head 12: [1.         0.99819946 1.0018616  ... 0.99935514 1.00079    0.9994728 ]\n",
      "Layer 19, Head 13: [1.        1.0015259 1.0013123 ... 1.0005643 0.9995565 1.0002617]\n",
      "Layer 19, Head 14: [1.         0.9987793  0.99890137 ... 0.99938774 0.9995353  0.9999323 ]\n",
      "Layer 19, Head 15: [1.        1.0014648 1.0015259 ... 1.0000883 0.9997424 1.0006704]\n",
      "Layer 19, Head 16: [1.         1.0002441  1.0009766  ... 0.99900126 0.99876267 0.9987959 ]\n",
      "Layer 19, Head 17: [1.        1.0009766 1.0006104 ... 0.9987428 0.9982694 0.9996815]\n",
      "Layer 19, Head 18: [1.         0.99920654 0.9998779  ... 1.0008823  0.9997429  0.9985266 ]\n",
      "Layer 19, Head 19: [1.        1.0009155 1.0018005 ... 1.0019501 0.999416  1.0018129]\n",
      "Layer 19, Head 20: [1.        0.9980469 1.0010376 ... 0.9984559 0.9989838 1.0001991]\n",
      "Layer 19, Head 21: [1.         0.9984741  1.0016174  ... 0.9999139  0.99920523 0.99876356]\n",
      "Layer 19, Head 22: [1.         0.99819946 0.99823    ... 0.99949    0.999482   1.0014817 ]\n",
      "Layer 19, Head 23: [1.        0.998291  1.001831  ... 1.0001174 1.0001941 1.0005271]\n",
      "Layer 19, Head 24: [1.         0.99938965 0.99890137 ... 1.0002863  1.0009799  1.0007768 ]\n",
      "Layer 19, Head 25: [1.         0.9996948  0.9998779  ... 1.00148    0.99959433 1.0003116 ]\n",
      "Layer 19, Head 26: [1.         1.0003662  0.9998779  ... 1.001046   0.9983873  0.99859524]\n",
      "Layer 19, Head 27: [1.         0.99816895 0.9984665  ... 1.0010164  0.99982077 1.0008627 ]\n",
      "Layer 19, Head 28: [1.        1.0002441 1.0012207 ... 1.0003563 0.9981878 1.0005882]\n",
      "Layer 19, Head 29: [1.         0.9986572  1.001709   ... 0.9988029  0.99832386 0.99840206]\n",
      "Layer 19, Head 30: [1.         1.0004883  1.0003052  ... 0.9987263  0.99818105 0.99946076]\n",
      "Layer 19, Head 31: [1.         0.9995117  0.99954224 ... 0.99858665 1.001668   1.0016851 ]\n",
      "Layer 20, Head 0: [1.        1.0019531 1.0019531 ... 1.0003446 1.0004363 0.9999907]\n",
      "Layer 20, Head 1: [1.         0.99902344 1.0006714  ... 1.000501   0.99944425 1.0003333 ]\n",
      "Layer 20, Head 2: [1.         1.0009766  1.0014343  ... 0.99810565 0.99973744 1.0009879 ]\n",
      "Layer 20, Head 3: [1.         1.0014648  0.99834824 ... 1.0011568  0.9998727  1.0004566 ]\n",
      "Layer 20, Head 4: [1.        1.0010681 1.0018921 ... 0.9985493 1.0014293 1.0017828]\n",
      "Layer 20, Head 5: [1.         0.99868774 0.9997864  ... 1.0001868  0.9990426  0.99919665]\n",
      "Layer 20, Head 6: [1.         0.99975586 1.0003204  ... 1.0005183  1.0007093  0.999912  ]\n",
      "Layer 20, Head 7: [1.         0.9980469  1.0011597  ... 0.99886274 1.0012769  1.0005896 ]\n",
      "Layer 20, Head 8: [1.         0.99853516 1.0014648  ... 0.9997214  1.0013797  1.0017805 ]\n",
      "Layer 20, Head 9: [1.         1.0018921  0.99948883 ... 0.99888515 0.998878   1.0004478 ]\n",
      "Layer 20, Head 10: [1.         1.0003662  0.99954224 ... 1.0006671  1.0004536  0.99807084]\n",
      "Layer 20, Head 11: [1.         1.0008545  0.99942017 ... 1.0010772  0.9995305  1.0015371 ]\n",
      "Layer 20, Head 12: [1.         1.0002441  0.9991455  ... 0.9995712  0.99890715 0.9982326 ]\n",
      "Layer 20, Head 13: [1.         1.0017395  0.9993286  ... 1.0006307  0.99821216 1.0013564 ]\n",
      "Layer 20, Head 14: [1.         1.001709   0.99816895 ... 0.9992713  0.99855435 0.9999488 ]\n",
      "Layer 20, Head 15: [1.         1.0011292  1.0014496  ... 0.99897784 0.99935365 0.99993587]\n",
      "Layer 20, Head 16: [1.         0.9998169  1.000824   ... 1.0009739  0.99917006 0.99940646]\n",
      "Layer 20, Head 17: [1.         0.99902344 0.99938965 ... 0.99942017 1.0015662  0.9985497 ]\n",
      "Layer 20, Head 18: [1.         0.999176   1.0016174  ... 0.9995526  0.99842244 0.999274  ]\n",
      "Layer 20, Head 19: [1.        0.9992676 1.0004883 ... 0.9984807 0.9998844 0.9984088]\n",
      "Layer 20, Head 20: [1.         0.9983063  1.0015144  ... 0.99921083 0.99978924 0.99820566]\n",
      "Layer 20, Head 21: [1.        1.0011597 1.0001068 ... 1.0011861 1.0010601 1.0004276]\n",
      "Layer 20, Head 22: [1.         0.99954224 1.0002823  ... 0.99979365 0.99830425 0.9998944 ]\n",
      "Layer 20, Head 23: [1.         1.0006104  0.9990015  ... 1.0008895  0.99921286 0.99901384]\n",
      "Layer 20, Head 24: [1.        0.998291  1.0003357 ... 0.9990934 0.9996177 0.9983027]\n",
      "Layer 20, Head 25: [1.         0.9989929  0.9998169  ... 0.99963224 1.0004787  0.9998071 ]\n",
      "Layer 20, Head 26: [1.         0.9990845  0.99938965 ... 1.0003419  0.9994638  1.001501  ]\n",
      "Layer 20, Head 27: [1.        1.0010376 1.0010376 ... 1.0009047 0.9992827 1.0004606]\n",
      "Layer 20, Head 28: [1.         0.998291   1.0015869  ... 1.0004684  0.99865913 0.99961454]\n",
      "Layer 20, Head 29: [1.         0.9986572  0.9995117  ... 0.99928796 0.99982387 1.0008044 ]\n",
      "Layer 20, Head 30: [1.         0.99902344 0.9983387  ... 1.0004591  1.0001811  1.0015901 ]\n",
      "Layer 20, Head 31: [1.        1.0004883 1.0005188 ... 0.9986216 1.0013804 1.0009518]\n",
      "Layer 21, Head 0: [1.         1.0007324  0.9995117  ... 1.0000228  0.9981982  0.99914074]\n",
      "Layer 21, Head 1: [1.         0.9992676  0.99890137 ... 0.99904704 1.0016487  1.0012125 ]\n",
      "Layer 21, Head 2: [1.         0.9987793  1.0010681  ... 1.0018089  0.99954367 1.0017174 ]\n",
      "Layer 21, Head 3: [1.         1.         0.99871826 ... 1.0010546  1.0002319  1.0003325 ]\n",
      "Layer 21, Head 4: [1.        1.0009766 1.0004272 ... 1.0010233 1.0000572 1.0008433]\n",
      "Layer 21, Head 5: [1.         0.9996338  1.0012512  ... 1.0013615  0.99971545 1.000936  ]\n",
      "Layer 21, Head 6: [1.        0.9996338 0.9985962 ... 0.9999492 0.9991689 1.0019723]\n",
      "Layer 21, Head 7: [1.         1.0001221  1.001709   ... 0.9986156  0.99996316 1.0016699 ]\n",
      "Layer 21, Head 8: [1.        1.0004883 1.0008545 ... 1.000512  0.9990276 1.0004047]\n",
      "Layer 21, Head 9: [1.         0.99853516 0.998291   ... 0.9991207  1.0009363  0.9986651 ]\n",
      "Layer 21, Head 10: [1.        1.        0.9995117 ... 1.0002954 1.0019147 1.0002888]\n",
      "Layer 21, Head 11: [1.         1.0003662  0.99918365 ... 1.0007575  0.9994211  1.0013652 ]\n",
      "Layer 21, Head 12: [1.         0.9998779  0.9989929  ... 1.0003023  0.9995598  0.99833775]\n",
      "Layer 21, Head 13: [1.         0.9992676  0.99838257 ... 1.0008849  1.0009763  1.0010723 ]\n",
      "Layer 21, Head 14: [1.         0.99853516 1.0011749  ... 1.0001669  0.9984431  0.99968874]\n",
      "Layer 21, Head 15: [1.         1.         0.9986572  ... 1.0018414  1.0009242  0.99912226]\n",
      "Layer 21, Head 16: [1.         0.99816895 0.99853516 ... 0.9999505  1.0015984  1.0003821 ]\n",
      "Layer 21, Head 17: [1.         1.0009766  1.001709   ... 0.99957645 1.0000048  0.9993331 ]\n",
      "Layer 21, Head 18: [1.         0.9986572  1.0003662  ... 1.0003383  0.99987006 1.0004132 ]\n",
      "Layer 21, Head 19: [1.         1.0004883  0.998291   ... 1.0015321  0.99944973 1.0004525 ]\n",
      "Layer 21, Head 20: [1.        1.0018921 0.9980469 ... 0.9984144 0.9982769 0.999787 ]\n",
      "Layer 21, Head 21: [1.        0.9987793 0.9984741 ... 1.0003574 1.0002089 1.0006915]\n",
      "Layer 21, Head 22: [1.         0.99868774 1.0012512  ... 1.0006073  0.999428   1.0006349 ]\n",
      "Layer 21, Head 23: [1.        1.0010986 0.9984741 ... 0.9990246 1.000634  0.9989538]\n",
      "Layer 21, Head 24: [1.         0.9996948  0.99990845 ... 1.0010986  1.0017848  0.9997293 ]\n",
      "Layer 21, Head 25: [1.        1.0000916 0.9996338 ... 1.0005431 0.998368  0.9982276]\n",
      "Layer 21, Head 26: [1.        0.9992981 1.0010071 ... 0.9995263 1.0015926 1.0006698]\n",
      "Layer 21, Head 27: [1.         1.0012207  0.999115   ... 1.0006288  0.99931985 1.0001239 ]\n",
      "Layer 21, Head 28: [1.         0.9984741  0.9980774  ... 0.9995043  0.9998833  0.99904716]\n",
      "Layer 21, Head 29: [1.        0.9984131 1.0010262 ... 1.0007069 1.0004    0.9988291]\n",
      "Layer 21, Head 30: [1.         1.0000305  0.99949646 ... 0.99814576 0.9994648  1.0014498 ]\n",
      "Layer 21, Head 31: [1.        1.0015488 0.9985218 ... 1.0013864 1.0011424 1.000826 ]\n",
      "Layer 22, Head 0: [1.         1.0019226  0.99961853 ... 1.000854   0.9983603  0.9984197 ]\n",
      "Layer 22, Head 1: [1.        1.0004883 1.0007477 ... 1.0012625 0.9981234 1.0000496]\n",
      "Layer 22, Head 2: [1.         0.99975586 0.99850464 ... 0.99956584 1.0012332  1.0016699 ]\n",
      "Layer 22, Head 3: [1.         0.99975586 0.99902344 ... 0.9993949  1.0006067  0.99849993]\n",
      "Layer 22, Head 4: [1.         0.99975586 0.998291   ... 1.0000075  0.99996465 0.9999814 ]\n",
      "Layer 22, Head 5: [1.         0.99868774 1.0007324  ... 0.9984057  0.9992503  1.0005695 ]\n",
      "Layer 22, Head 6: [1.         1.0015869  1.0006104  ... 1.0007463  0.99818194 1.0001013 ]\n",
      "Layer 22, Head 7: [1.         1.0006104  0.9995117  ... 0.9995737  0.99968785 0.99821055]\n",
      "Layer 22, Head 8: [1.        1.0003662 1.0005798 ... 1.0016263 1.0015943 1.0017611]\n",
      "Layer 22, Head 9: [1.         0.9980469  1.         ... 0.99945635 0.9986659  0.9994501 ]\n",
      "Layer 22, Head 10: [1.         0.99823    0.9994812  ... 1.0012268  0.9993895  0.99808246]\n",
      "Layer 22, Head 11: [1.         1.0004883  0.9995117  ... 0.9993617  0.9987691  0.99896216]\n",
      "Layer 22, Head 12: [1.         1.0019226  0.99925995 ... 1.0003679  1.0018044  0.9998568 ]\n",
      "Layer 22, Head 13: [1.         1.         1.0007629  ... 1.0018418  1.0007423  0.99903345]\n",
      "Layer 22, Head 14: [1.        1.0019531 1.0009766 ... 0.9999123 0.9986294 1.0014281]\n",
      "Layer 22, Head 15: [1.        0.9984131 0.99823   ... 1.0014987 0.9981868 0.9991435]\n",
      "Layer 22, Head 16: [1.         0.99902344 0.99902344 ... 0.9999937  0.99863654 0.9986077 ]\n",
      "Layer 22, Head 17: [1.         0.9996338  1.0014648  ... 0.99832475 1.0008667  0.99957937]\n",
      "Layer 22, Head 18: [1.        1.        0.9986572 ... 1.0015368 1.0001788 1.0014992]\n",
      "Layer 22, Head 19: [1.         1.         0.9979248  ... 1.0017555  1.0008619  0.99907094]\n",
      "Layer 22, Head 20: [1.         0.99975586 0.9999695  ... 1.0018294  0.99935293 0.999593  ]\n",
      "Layer 22, Head 21: [1.         0.9992676  1.0017548  ... 1.0009531  0.99999833 0.9992866 ]\n",
      "Layer 22, Head 22: [1.         0.9990845  0.9992523  ... 1.0006762  0.99811876 0.9988385 ]\n",
      "Layer 22, Head 23: [1.         1.0009766  1.0009766  ... 0.9987372  0.9980552  0.99973017]\n",
      "Layer 22, Head 24: [1.         0.9984131  1.0002441  ... 0.99999857 1.0016423  1.0009239 ]\n",
      "Layer 22, Head 25: [1.        1.0014648 1.0004883 ... 0.9983969 0.9998018 0.9985328]\n",
      "Layer 22, Head 26: [1.         1.0006104  0.9993286  ... 0.9997343  0.99839574 0.99971366]\n",
      "Layer 22, Head 27: [1.         0.99853516 0.9990845  ... 1.0013992  0.99963117 1.0002266 ]\n",
      "Layer 22, Head 28: [1.        1.0007324 1.0015717 ... 1.000284  1.0013704 1.0000944]\n",
      "Layer 22, Head 29: [1.         0.99890137 0.99975586 ... 0.9983513  0.999975   1.0016268 ]\n",
      "Layer 22, Head 30: [1.         1.0017395  1.0001526  ... 0.9988196  1.0014635  0.99936754]\n",
      "Layer 22, Head 31: [1.         1.001831   1.0002594  ... 0.99959946 0.9990775  1.0017936 ]\n",
      "Layer 23, Head 0: [1.        1.0011597 0.9986267 ... 0.9995648 1.0008566 0.999945 ]\n",
      "Layer 23, Head 1: [1.         1.         0.99975586 ... 0.99983346 0.99927956 0.9999114 ]\n",
      "Layer 23, Head 2: [1.         0.9985962  1.00177    ... 1.0011848  0.99858284 0.9990637 ]\n",
      "Layer 23, Head 3: [1.        0.998291  0.9984131 ... 1.0012602 0.9982891 1.00148  ]\n",
      "Layer 23, Head 4: [1.         0.99838257 0.99923706 ... 1.0015731  0.9982537  1.0013049 ]\n",
      "Layer 23, Head 5: [1.        1.0003052 0.9985657 ... 0.9991386 0.9981122 1.0003624]\n",
      "Layer 23, Head 6: [1.         0.99838257 1.0002441  ... 0.998724   1.0006269  0.9995061 ]\n",
      "Layer 23, Head 7: [1.        1.0002441 1.0007782 ... 1.0006971 1.0007775 0.9992118]\n",
      "Layer 23, Head 8: [1.         1.0004883  0.99887085 ... 0.9994288  1.0008872  1.0013123 ]\n",
      "Layer 23, Head 9: [1.         1.0008545  0.99890137 ... 0.9998549  1.0007654  0.99902457]\n",
      "Layer 23, Head 10: [1.         0.99902344 1.0014648  ... 1.0011544  1.0004747  1.0006976 ]\n",
      "Layer 23, Head 11: [1.         0.9995117  1.0009766  ... 0.9991543  1.0016291  0.99873567]\n",
      "Layer 23, Head 12: [1.         0.9998779  0.9989052  ... 1.0000341  0.999875   0.99953425]\n",
      "Layer 23, Head 13: [1.         1.0009766  0.9995537  ... 1.0001174  0.99900144 1.0004371 ]\n",
      "Layer 23, Head 14: [1.        1.0002441 1.0006104 ... 0.9983825 1.0001229 1.00155  ]\n",
      "Layer 23, Head 15: [1.        1.0010681 1.001648  ... 1.0016212 0.9990411 0.9989989]\n",
      "Layer 23, Head 16: [1.        1.0003662 1.0010681 ... 0.9999551 1.0001092 1.0008771]\n",
      "Layer 23, Head 17: [1.         0.99838257 0.99871826 ... 0.99937606 1.0003653  1.0003128 ]\n",
      "Layer 23, Head 18: [1.         1.0011597  0.99938965 ... 0.99869436 1.000895   0.99921626]\n",
      "Layer 23, Head 19: [1.         0.9993744  1.         ... 0.99849224 0.99961936 0.9997487 ]\n",
      "Layer 23, Head 20: [1.        1.0015869 0.9994507 ... 1.0001483 1.0019388 1.0016025]\n",
      "Layer 23, Head 21: [1.         1.0003662  0.99838257 ... 0.998713   0.9984466  1.001457  ]\n",
      "Layer 23, Head 22: [1.         1.0010986  0.9990158  ... 1.0004817  0.9980939  0.99882275]\n",
      "Layer 23, Head 23: [1.         0.9981079  0.99920654 ... 1.0004141  1.0007051  1.0011462 ]\n",
      "Layer 23, Head 24: [1.         0.998291   0.99884033 ... 0.9998535  1.0015709  1.0005478 ]\n",
      "Layer 23, Head 25: [1.        1.        0.99823   ... 0.9987528 0.9992076 0.9992819]\n",
      "Layer 23, Head 26: [1.         1.0009766  1.0007324  ... 1.0018101  1.0003512  0.99986684]\n",
      "Layer 23, Head 27: [1.         0.99938965 0.9995575  ... 0.998767   1.0003617  1.0006131 ]\n",
      "Layer 23, Head 28: [1.        1.0013428 0.9993286 ... 1.001687  1.0016834 1.000569 ]\n",
      "Layer 23, Head 29: [1.         0.9986572  1.0003662  ... 0.99990505 1.000944   0.9992673 ]\n",
      "Layer 23, Head 30: [1.        0.9998779 0.9986572 ... 0.9999805 0.9985782 1.0007432]\n",
      "Layer 23, Head 31: [1.         0.99975586 1.0002441  ... 0.9993198  1.0015683  1.0001593 ]\n",
      "Layer 24, Head 0: [1.        0.9992676 1.0007935 ... 0.9994113 0.9987483 1.0005643]\n",
      "Layer 24, Head 1: [1.         0.99975586 0.9988098  ... 1.0014781  0.9999568  0.9991047 ]\n",
      "Layer 24, Head 2: [1.         1.0019531  0.9980469  ... 1.0011247  0.99868387 1.0000908 ]\n",
      "Layer 24, Head 3: [1.         1.0014648  1.0013275  ... 0.9999004  0.99890965 0.99985135]\n",
      "Layer 24, Head 4: [1.         0.9992676  0.99868774 ... 1.0018476  1.0016557  0.99813956]\n",
      "Layer 24, Head 5: [1.         0.9986572  0.99853516 ... 1.0006673  0.999305   1.0001106 ]\n",
      "Layer 24, Head 6: [1.        1.0014648 1.001709  ... 1.001337  1.0015334 1.0016506]\n",
      "Layer 24, Head 7: [1.         0.99853516 1.0006714  ... 1.000484   0.9998159  1.0005572 ]\n",
      "Layer 24, Head 8: [1.         1.         0.99861145 ... 0.9988294  1.0015913  1.0003155 ]\n",
      "Layer 24, Head 9: [1.         0.99890137 0.99938965 ... 0.9993086  0.99948376 0.9999558 ]\n",
      "Layer 24, Head 10: [1.        1.0009766 0.9999695 ... 1.0019308 0.9986807 1.0000113]\n",
      "Layer 24, Head 11: [1.        1.001709  1.0010376 ... 1.0011736 1.0002929 1.0000763]\n",
      "Layer 24, Head 12: [1.         1.0001221  0.99871826 ... 0.99866927 1.0013336  1.0007997 ]\n",
      "Layer 24, Head 13: [1.         1.001709   1.0014648  ... 0.99932426 1.0003071  0.99971175]\n",
      "Layer 24, Head 14: [1.        1.0019531 1.        ... 0.999655  1.000581  0.9981996]\n",
      "Layer 24, Head 15: [1.         0.9992981  0.99853516 ... 1.0000229  1.000261   0.99926597]\n",
      "Layer 24, Head 16: [1.         0.99897766 1.000557   ... 0.9995447  1.0006632  0.99983263]\n",
      "Layer 24, Head 17: [1.         0.9985657  0.9992981  ... 1.0002989  1.0006884  0.99880683]\n",
      "Layer 24, Head 18: [1.         1.0013123  1.0017548  ... 0.9988485  0.9991054  0.99994063]\n",
      "Layer 24, Head 19: [1.         0.999176   0.9992523  ... 0.99924624 1.001525   0.99843234]\n",
      "Layer 24, Head 20: [1.         1.0018921  1.0012054  ... 0.9990595  1.0018228  0.99883664]\n",
      "Layer 24, Head 21: [1.        0.9993744 1.001236  ... 0.9993233 0.9979986 1.0005503]\n",
      "Layer 24, Head 22: [1.         0.9990845  0.99974823 ... 0.99820065 0.99832886 0.9988469 ]\n",
      "Layer 24, Head 23: [1.        0.9998779 1.0010071 ... 0.9998398 1.0018322 0.9999474]\n",
      "Layer 24, Head 24: [1.         1.00177    0.99909973 ... 1.0006849  1.0019089  1.0013367 ]\n",
      "Layer 24, Head 25: [1.         0.9980469  0.99983215 ... 1.0009181  0.9995958  0.9985139 ]\n",
      "Layer 24, Head 26: [1.         1.0014648  0.9994278  ... 1.0013684  0.99962467 1.0003219 ]\n",
      "Layer 24, Head 27: [1.         0.99938965 1.0011749  ... 0.99974835 1.0013235  0.998763  ]\n",
      "Layer 24, Head 28: [1.         0.9984741  1.0014648  ... 0.99970675 1.0005336  0.99952483]\n",
      "Layer 24, Head 29: [1.         0.9992676  0.9998779  ... 0.9991474  0.9992498  0.99865913]\n",
      "Layer 24, Head 30: [1.         1.0003662  0.99816895 ... 0.9987919  1.000182   0.99978733]\n",
      "Layer 24, Head 31: [1.         0.99868774 0.9981079  ... 1.0001051  1.0002807  0.9994626 ]\n",
      "Layer 25, Head 0: [1.         0.9996338  0.9996948  ... 0.9987496  1.0008543  0.99961597]\n",
      "Layer 25, Head 1: [1.         0.99993896 1.001709   ... 1.0002778  0.99922264 0.99827135]\n",
      "Layer 25, Head 2: [1.        1.0011597 1.0009155 ... 0.9994629 1.0005378 1.0000677]\n",
      "Layer 25, Head 3: [1.         0.9998779  0.99975586 ... 0.999787   1.0002836  0.9982215 ]\n",
      "Layer 25, Head 4: [1.         1.0003052  0.99882317 ... 0.99837977 0.99888563 1.0009046 ]\n",
      "Layer 25, Head 5: [1.        1.0012817 0.998476  ... 0.9991773 1.0003029 1.0017061]\n",
      "Layer 25, Head 6: [1.         1.0010986  0.99858856 ... 1.0017946  1.00024    0.99899924]\n",
      "Layer 25, Head 7: [1.         1.0010681  0.99872684 ... 1.0007529  1.0013217  1.000771  ]\n",
      "Layer 25, Head 8: [1.         0.99890137 1.0009766  ... 1.000857   0.99963796 0.9980513 ]\n",
      "Layer 25, Head 9: [1.        1.0014648 0.9992676 ... 1.0003859 1.0000616 0.9994676]\n",
      "Layer 25, Head 10: [1.         0.99938965 1.0012207  ... 0.9996818  1.0002469  0.9992616 ]\n",
      "Layer 25, Head 11: [1.         0.9989624  1.0014648  ... 0.99947596 0.9993435  1.0016444 ]\n",
      "Layer 25, Head 12: [1.         1.0015869  1.0014515  ... 0.99932027 0.99993014 0.9983276 ]\n",
      "Layer 25, Head 13: [1.         1.0007935  0.9994354  ... 1.0002903  0.99992543 1.0014586 ]\n",
      "Layer 25, Head 14: [1.        0.9981079 1.0000916 ... 1.0010743 1.0006416 1.0016462]\n",
      "Layer 25, Head 15: [1.         0.9996338  1.0003586  ... 1.0011384  0.9982613  0.99835277]\n",
      "Layer 25, Head 16: [1.        1.001709  1.0009766 ... 1.0011337 1.0007026 1.0016646]\n",
      "Layer 25, Head 17: [1.        1.0002441 0.9980469 ... 1.000182  0.9983369 1.000632 ]\n",
      "Layer 25, Head 18: [1.         1.0009766  0.99835205 ... 0.9994727  1.0000143  0.9988861 ]\n",
      "Layer 25, Head 19: [1.        1.0009766 1.0014648 ... 1.0014575 1.0007023 0.9985026]\n",
      "Layer 25, Head 20: [1.         1.0003662  0.99902344 ... 1.0013853  1.0001042  1.0003197 ]\n",
      "Layer 25, Head 21: [1.         1.0019531  1.0006104  ... 0.99836373 1.0005152  1.0000417 ]\n",
      "Layer 25, Head 22: [1.        1.0009766 0.9991455 ... 1.0012023 0.9987889 0.9985751]\n",
      "Layer 25, Head 23: [1.         1.0018921  0.9998169  ... 0.9991262  0.9995546  0.99876755]\n",
      "Layer 25, Head 24: [1.        1.0014648 1.0006714 ... 0.9993789 0.9992328 1.0015559]\n",
      "Layer 25, Head 25: [1.        0.9992676 1.0012207 ... 0.9986745 0.9987489 1.0006051]\n",
      "Layer 25, Head 26: [1.        0.9996338 1.00177   ... 1.0001143 1.0004778 1.0015476]\n",
      "Layer 25, Head 27: [1.         1.0005493  1.0001831  ... 1.0002952  0.99975777 1.0003409 ]\n",
      "Layer 25, Head 28: [1.         0.9996338  1.0001831  ... 0.99986565 0.9984241  0.99922264]\n",
      "Layer 25, Head 29: [1.         1.0006104  0.9982605  ... 1.0017362  1.0004082  0.99860924]\n",
      "Layer 25, Head 30: [1.        1.        1.0009613 ... 1.0010223 1.0015761 0.9982872]\n",
      "Layer 25, Head 31: [1.         0.99998474 1.0003357  ... 1.0000688  0.99896157 0.9988382 ]\n",
      "Layer 26, Head 0: [1.        1.0014648 0.9975281 ... 0.9988838 1.0008936 0.998885 ]\n",
      "Layer 26, Head 1: [1.         1.         0.9974365  ... 1.000604   0.99865377 1.0017602 ]\n",
      "Layer 26, Head 2: [1.         1.0002441  1.000473   ... 0.99934566 0.99954385 1.000143  ]\n",
      "Layer 26, Head 3: [1.         0.99938965 0.99868774 ... 1.000629   0.9988294  1.0012678 ]\n",
      "Layer 26, Head 4: [1.         1.         0.99975586 ... 0.99800766 1.0003034  1.0000806 ]\n",
      "Layer 26, Head 5: [1.        1.001709  1.0014038 ... 0.9988488 1.0000399 1.0001254]\n",
      "Layer 26, Head 6: [1.        1.0015259 0.9993286 ... 1.000386  0.998752  1.0000923]\n",
      "Layer 26, Head 7: [1.         1.001709   0.99890137 ... 0.99978054 0.9996042  0.99814177]\n",
      "Layer 26, Head 8: [1.        1.        1.0010376 ... 0.9998603 1.0004442 0.9985795]\n",
      "Layer 26, Head 9: [1.         0.9986572  0.99902344 ... 1.0010679  0.9993206  0.9986534 ]\n",
      "Layer 26, Head 10: [1.         0.9998779  1.0015869  ... 1.0005037  0.99920976 1.0004234 ]\n",
      "Layer 26, Head 11: [1.        1.0002441 1.0001221 ... 1.0005199 0.9994084 1.0000888]\n",
      "Layer 26, Head 12: [1.         1.0003662  1.0005646  ... 1.0004267  0.99984884 0.9989197 ]\n",
      "Layer 26, Head 13: [1.         0.99920654 0.9985962  ... 0.99899435 1.0000968  0.999559  ]\n",
      "Layer 26, Head 14: [1.        1.0012817 1.0017853 ... 1.0010967 1.0012141 0.9983733]\n",
      "Layer 26, Head 15: [1.        1.001709  0.9980469 ... 1.0002444 1.0002222 1.001914 ]\n",
      "Layer 26, Head 16: [1.         0.9985657  1.0000916  ... 1.0016721  0.99812084 1.0001874 ]\n",
      "Layer 26, Head 17: [1.         0.99890137 1.0001831  ... 0.99960804 0.99844813 0.9993104 ]\n",
      "Layer 26, Head 18: [1.        0.9994507 1.001831  ... 0.9983603 1.0000209 1.0006002]\n",
      "Layer 26, Head 19: [1.        1.0003052 0.9993286 ... 1.0019082 1.0006366 1.0002058]\n",
      "Layer 26, Head 20: [1.         1.0003662  0.99868774 ... 1.0012951  1.0000231  1.0013267 ]\n",
      "Layer 26, Head 21: [1.         1.0018921  1.0014038  ... 1.0016819  0.99900174 1.0002006 ]\n",
      "Layer 26, Head 22: [1.         1.0019531  0.99902344 ... 0.99961764 1.0004897  1.0000556 ]\n",
      "Layer 26, Head 23: [1.         0.99902344 1.0010986  ... 1.0013121  0.99802196 1.0001427 ]\n",
      "Layer 26, Head 24: [1.         1.0015259  0.99887085 ... 1.0005262  0.99843466 0.9999561 ]\n",
      "Layer 26, Head 25: [1.        1.0014648 1.0007935 ... 1.0009723 0.9988437 1.0004609]\n",
      "Layer 26, Head 26: [1.         1.0012207  0.99938965 ... 0.9993179  1.0012805  0.9985933 ]\n",
      "Layer 26, Head 27: [1.         1.0010376  0.9998474  ... 1.0005338  1.000716   0.99925256]\n",
      "Layer 26, Head 28: [1.         0.99975586 1.001831   ... 1.0002449  0.99933165 0.9985304 ]\n",
      "Layer 26, Head 29: [1.         1.00177    0.9988785  ... 0.99887645 1.0007441  0.9994995 ]\n",
      "Layer 26, Head 30: [1.        1.0004272 1.0004883 ... 1.0003058 1.0014877 1.0003529]\n",
      "Layer 26, Head 31: [1.         1.0004272  0.99980164 ... 0.9999633  0.99951    1.00157   ]\n",
      "Layer 27, Head 0: [1.         0.99957275 1.0017395  ... 1.0013694  1.0000058  1.0010077 ]\n",
      "Layer 27, Head 1: [1.         0.99920654 0.9984131  ... 1.0012214  1.0015299  0.9986215 ]\n",
      "Layer 27, Head 2: [1.         1.         1.0010986  ... 0.99970937 0.9992536  0.9992514 ]\n",
      "Layer 27, Head 3: [1.         1.0004883  1.0005188  ... 0.99874336 1.0019022  1.0010607 ]\n",
      "Layer 27, Head 4: [1.         1.0016708  0.9996624  ... 1.0018948  0.99852955 1.0018319 ]\n",
      "Layer 27, Head 5: [1.         0.9989929  1.0015106  ... 0.9982928  0.99824417 0.9983285 ]\n",
      "Layer 27, Head 6: [1.         1.001648   1.0006332  ... 1.0018754  1.001336   0.99940896]\n",
      "Layer 27, Head 7: [1.         0.9981079  0.9993057  ... 0.99828506 1.0014356  0.9987473 ]\n",
      "Layer 27, Head 8: [1.        0.9998627 1.0003662 ... 0.9996735 1.0009568 1.0003984]\n",
      "Layer 27, Head 9: [1.         1.0009155  1.0014038  ... 1.0001253  0.99987817 0.999206  ]\n",
      "Layer 27, Head 10: [1.         0.99816895 0.99942017 ... 1.001637   1.0005193  0.999352  ]\n",
      "Layer 27, Head 11: [1.         1.0004883  0.99957275 ... 0.99982923 1.0003637  0.9994247 ]\n",
      "Layer 27, Head 12: [1.         1.0009766  0.9980469  ... 0.99854255 1.0017799  1.0009072 ]\n",
      "Layer 27, Head 13: [1.         0.99902344 1.0010376  ... 0.99841255 1.000641   0.9983357 ]\n",
      "Layer 27, Head 14: [1.        1.001709  1.0009766 ... 0.999786  1.0011921 1.0004218]\n",
      "Layer 27, Head 15: [1.         0.9980469  0.99902344 ... 1.000402   0.9995617  1.0000322 ]\n",
      "Layer 27, Head 16: [1.         0.9987793  0.99902344 ... 0.99927413 0.99979776 0.9987273 ]\n",
      "Layer 27, Head 17: [1.        0.9980469 1.0003052 ... 1.0007635 1.0006918 1.0014702]\n",
      "Layer 27, Head 18: [1.         0.99816895 1.0013428  ... 1.001459   1.0004222  1.0005617 ]\n",
      "Layer 27, Head 19: [1.         1.0014038  1.0008545  ... 0.9982828  0.9983678  0.99991447]\n",
      "Layer 27, Head 20: [1.        1.0010376 0.9989319 ... 1.0016123 0.9992091 0.9981234]\n",
      "Layer 27, Head 21: [1.         0.99816895 0.9999542  ... 1.0011628  1.000942   0.99905217]\n",
      "Layer 27, Head 22: [1.         0.9985657  0.9988251  ... 1.0001783  1.0015193  0.99973476]\n",
      "Layer 27, Head 23: [1.         0.99819946 1.0019531  ... 0.9982875  1.001331   1.0019727 ]\n",
      "Layer 27, Head 24: [1.         0.99890137 1.0007019  ... 0.99957263 0.9984189  1.0018257 ]\n",
      "Layer 27, Head 25: [1.        1.0018921 1.00177   ... 1.0009531 0.9992322 0.9984322]\n",
      "Layer 27, Head 26: [1.         0.99920654 0.99864197 ... 0.9987968  0.99856573 1.0017376 ]\n",
      "Layer 27, Head 27: [1.        0.9981384 0.9996567 ... 0.9987263 1.0010047 1.0007905]\n",
      "Layer 27, Head 28: [1.         1.0002441  0.9993286  ... 0.9994049  0.9986153  0.99905664]\n",
      "Layer 27, Head 29: [1.         0.99853516 0.9987793  ... 1.0010924  0.998611   1.0002931 ]\n",
      "Layer 27, Head 30: [1.         1.0004883  0.99871826 ... 1.00045    1.0009254  1.0000741 ]\n",
      "Layer 27, Head 31: [1.        1.0007324 1.0018616 ... 1.0013772 1.0007322 0.9983567]\n",
      "Layer 28, Head 0: [1.        0.9991455 0.9994507 ... 0.9986107 0.9988402 1.0011464]\n",
      "Layer 28, Head 1: [1.         1.0012207  1.         ... 0.99924517 0.9982937  1.0019199 ]\n",
      "Layer 28, Head 2: [1.        1.0009155 1.0003967 ... 1.000364  1.0003462 0.9982544]\n",
      "Layer 28, Head 3: [1.        1.        0.998291  ... 1.0009706 1.0016738 1.0013827]\n",
      "Layer 28, Head 4: [1.        0.9992981 1.0012207 ... 1.0006769 0.9985884 1.0003563]\n",
      "Layer 28, Head 5: [1.         1.0012207  0.99890137 ... 1.0013802  1.0017169  1.0010124 ]\n",
      "Layer 28, Head 6: [1.         0.9980469  1.         ... 0.9995037  0.99827266 1.0013703 ]\n",
      "Layer 28, Head 7: [1.         0.99902344 1.         ... 1.0010842  0.99913555 0.9997215 ]\n",
      "Layer 28, Head 8: [1.        0.9992676 1.0003662 ... 1.0018861 1.0011468 0.9987601]\n",
      "Layer 28, Head 9: [1.         0.9992676  0.9991455  ... 0.99901193 1.0009356  1.0010756 ]\n",
      "Layer 28, Head 10: [1.        1.0009766 1.0008545 ... 0.9992349 1.0011678 1.0005252]\n",
      "Layer 28, Head 11: [1.         0.99902344 1.0002441  ... 1.0004189  0.99991673 1.0012075 ]\n",
      "Layer 28, Head 12: [1.        1.0011597 0.9989319 ... 0.9998948 0.9994053 1.0015782]\n",
      "Layer 28, Head 13: [1.         1.001709   1.0010376  ... 1.0018266  0.99863106 0.99980474]\n",
      "Layer 28, Head 14: [1.        0.9989624 0.9993286 ... 1.0003593 0.9995056 1.0010815]\n",
      "Layer 28, Head 15: [1.         1.0007629  1.0002441  ... 0.99956197 1.0010772  1.0001264 ]\n",
      "Layer 28, Head 16: [1.         1.0002441  1.0012817  ... 1.0003724  0.99814266 1.0010655 ]\n",
      "Layer 28, Head 17: [1.         0.99902344 1.001831   ... 0.9984924  0.9998026  1.0010952 ]\n",
      "Layer 28, Head 18: [1.         0.9996338  0.9995117  ... 0.999224   0.99889314 0.9984071 ]\n",
      "Layer 28, Head 19: [1.         0.99853516 1.0003052  ... 0.9983243  1.001601   0.99885803]\n",
      "Layer 28, Head 20: [1.        1.0004883 1.0007935 ... 0.9979768 0.9976818 1.0017189]\n",
      "Layer 28, Head 21: [1.         0.99902344 1.         ... 1.0002943  0.9997045  1.0005281 ]\n",
      "Layer 28, Head 22: [1.         0.99902344 1.001709   ... 0.99803686 1.0017698  1.0017519 ]\n",
      "Layer 28, Head 23: [1.         1.0012207  1.0004272  ... 1.000229   0.99953216 1.0005349 ]\n",
      "Layer 28, Head 24: [1.         0.99853516 0.99954224 ... 1.0003545  1.0004705  0.9988062 ]\n",
      "Layer 28, Head 25: [1.        0.9993286 1.        ... 0.9983504 0.9991728 1.0017961]\n",
      "Layer 28, Head 26: [1.         1.0002441  0.9986572  ... 0.99927026 0.9996373  0.9995898 ]\n",
      "Layer 28, Head 27: [1.        0.9998169 0.9988556 ... 1.0019473 1.0009772 0.9996491]\n",
      "Layer 28, Head 28: [1.         0.99853516 0.99871826 ... 0.99958044 1.0019606  1.0007727 ]\n",
      "Layer 28, Head 29: [1.        1.0019531 1.0005493 ... 1.0002056 1.0001769 0.9998874]\n",
      "Layer 28, Head 30: [1.         1.0012207  0.9984131  ... 0.99931455 1.0015736  0.9991815 ]\n",
      "Layer 28, Head 31: [1.         1.0009766  0.999115   ... 1.0012064  0.99970675 0.9988609 ]\n",
      "Layer 29, Head 0: [1.         0.99902344 0.9985962  ... 1.0008637  1.0005301  1.0014412 ]\n",
      "Layer 29, Head 1: [1.        0.9980469 1.0004272 ... 0.9983195 1.0000579 1.0005425]\n",
      "Layer 29, Head 2: [1.         1.0004883  0.99938965 ... 1.00181    1.0017823  0.9989865 ]\n",
      "Layer 29, Head 3: [1.         1.         0.99890137 ... 1.0013758  1.000949   1.0015414 ]\n",
      "Layer 29, Head 4: [1.        0.999527  1.0009766 ... 1.0009464 1.0003686 1.0014778]\n",
      "Layer 29, Head 5: [1.         1.0014648  0.99853516 ... 1.00185    0.9981038  1.0002297 ]\n",
      "Layer 29, Head 6: [1.         1.0007324  1.0003662  ... 1.0006336  0.99834204 1.0017949 ]\n",
      "Layer 29, Head 7: [1.         0.99902344 1.0015259  ... 0.998615   1.0017326  0.99949974]\n",
      "Layer 29, Head 8: [1.        0.9996948 1.0006104 ... 0.998544  1.0019286 0.9988389]\n",
      "Layer 29, Head 9: [1.         0.998291   0.99916077 ... 0.99850714 1.0010334  0.9997538 ]\n",
      "Layer 29, Head 10: [1.        1.        0.998291  ... 1.0008211 1.0004455 0.9991923]\n",
      "Layer 29, Head 11: [1.         0.9995117  1.0001221  ... 0.99859226 0.99981284 0.9984648 ]\n",
      "Layer 29, Head 12: [1.         1.0012207  0.99884033 ... 1.0002743  1.0008831  1.0015653 ]\n",
      "Layer 29, Head 13: [1.         1.         0.99871826 ... 1.0009534  1.0008545  1.001434  ]\n",
      "Layer 29, Head 14: [1.        0.9998627 1.0010986 ... 0.9988331 1.0003424 0.9991488]\n",
      "Layer 29, Head 15: [1.         0.9992981  1.00177    ... 0.9991041  0.99997234 0.99935526]\n",
      "Layer 29, Head 16: [1.         1.0004883  1.         ... 0.99895465 0.9984362  1.0018439 ]\n",
      "Layer 29, Head 17: [1.         1.001709   0.9995117  ... 0.99924195 1.0003896  1.0015339 ]\n",
      "Layer 29, Head 18: [1.         0.99902344 1.         ... 0.9980749  1.000245   0.999214  ]\n",
      "Layer 29, Head 19: [1.         0.9980469  0.99816895 ... 0.99848855 0.9992833  0.99998116]\n",
      "Layer 29, Head 20: [1.         0.9992676  1.         ... 0.99842334 1.0000687  1.0013304 ]\n",
      "Layer 29, Head 21: [1.         1.0015869  0.99975586 ... 0.9999666  1.0011027  1.0007565 ]\n",
      "Layer 29, Head 22: [1.        0.9987793 1.001648  ... 0.9996407 1.0000767 1.0002651]\n",
      "Layer 29, Head 23: [1.         1.001709   0.99975586 ... 1.000144   1.0001464  0.9999183 ]\n",
      "Layer 29, Head 24: [1.         1.0007935  1.0008545  ... 0.999895   0.99832475 1.0001568 ]\n",
      "Layer 29, Head 25: [1.         1.0009766  0.99902344 ... 0.9989768  1.0002358  0.9998121 ]\n",
      "Layer 29, Head 26: [1.        1.0007324 0.999115  ... 1.0013062 0.9997995 1.0011593]\n",
      "Layer 29, Head 27: [1.        0.9980469 1.0005188 ... 1.0000407 1.0015231 0.9985398]\n",
      "Layer 29, Head 28: [1.         1.0006104  0.99838257 ... 0.9980464  0.9982728  0.99818075]\n",
      "Layer 29, Head 29: [1.        0.9991455 1.001709  ... 1.0019102 0.9997102 0.9995254]\n",
      "Layer 29, Head 30: [1.         0.99975586 0.9992676  ... 1.0016447  1.0003835  0.99843246]\n",
      "Layer 29, Head 31: [1.         0.99902344 0.9991455  ... 1.0005277  1.0015852  1.0013956 ]\n",
      "Layer 30, Head 0: [1.         1.         0.99902344 ... 1.000284   0.99958605 0.9995944 ]\n",
      "Layer 30, Head 1: [1.        0.9980469 1.001831  ... 0.997838  1.0004814 1.0005853]\n",
      "Layer 30, Head 2: [1.         0.9995117  0.9995117  ... 0.9983458  0.9992837  0.99973553]\n",
      "Layer 30, Head 3: [1.         1.0015869  0.99935913 ... 0.9998058  0.9987992  1.0005488 ]\n",
      "Layer 30, Head 4: [1.         0.9989624  0.99838257 ... 0.9984809  1.0009937  1.0003865 ]\n",
      "Layer 30, Head 5: [1.         0.9995117  0.99902344 ... 1.0006258  0.99927974 1.0006334 ]\n",
      "Layer 30, Head 6: [1.         0.99938965 0.9988098  ... 1.00194    1.0013114  1.0018089 ]\n",
      "Layer 30, Head 7: [1.         0.99853516 0.9980469  ... 0.99996316 1.0016527  0.9989709 ]\n",
      "Layer 30, Head 8: [1.        0.9992676 1.0014648 ... 0.9987161 1.0007339 1.0000563]\n",
      "Layer 30, Head 9: [1.         0.99902344 1.0003357  ... 1.0006558  1.0020568  0.9989171 ]\n",
      "Layer 30, Head 10: [1.         0.998291   0.998291   ... 1.0000021  0.99862325 1.0004165 ]\n",
      "Layer 30, Head 11: [1.         0.9998779  0.99816895 ... 1.0009468  1.001748   0.9995255 ]\n",
      "Layer 30, Head 12: [1.        0.9992676 1.0001373 ... 1.0011222 0.999787  1.0000422]\n",
      "Layer 30, Head 13: [1.        1.0004883 1.0002136 ... 1.0006708 1.0001341 1.0005897]\n",
      "Layer 30, Head 14: [1.         1.0009766  1.001709   ... 1.000973   0.99955404 0.99918246]\n",
      "Layer 30, Head 15: [1.         1.0006104  1.0003967  ... 0.99870056 0.9994948  1.0002968 ]\n",
      "Layer 30, Head 16: [1.         1.0004883  1.0012207  ... 1.0004296  0.99951994 1.000591  ]\n",
      "Layer 30, Head 17: [1.        1.        1.0014648 ... 1.0009351 1.0003153 1.0013512]\n",
      "Layer 30, Head 18: [1.         1.0019531  0.99902344 ... 0.9999339  0.99923    0.99852526]\n",
      "Layer 30, Head 19: [1.        1.0004883 0.9987793 ... 1.0003042 1.0000079 1.0007493]\n",
      "Layer 30, Head 20: [1.         0.9980469  1.001709   ... 1.0005159  0.9999452  0.99922734]\n",
      "Layer 30, Head 21: [1.        1.0012207 0.9999237 ... 0.9983174 1.0013996 0.9983482]\n",
      "Layer 30, Head 22: [1.         1.0009766  0.9987793  ... 1.000115   0.99872077 1.001162  ]\n",
      "Layer 30, Head 23: [1.         0.99902344 1.0015869  ... 1.0000583  1.0009346  0.9999423 ]\n",
      "Layer 30, Head 24: [1.         0.99853516 0.9992676  ... 0.9987109  1.0005329  1.0008798 ]\n",
      "Layer 30, Head 25: [1.         1.         1.0002441  ... 0.99876356 0.9985187  0.9989383 ]\n",
      "Layer 30, Head 26: [1.         0.99816895 1.0003052  ... 0.99957436 0.9997983  0.99888057]\n",
      "Layer 30, Head 27: [1.        1.        1.0012207 ... 1.0015622 1.0018083 1.0001321]\n",
      "Layer 30, Head 28: [1.         1.0015564  0.99819946 ... 0.9995451  0.9998082  0.9998284 ]\n",
      "Layer 30, Head 29: [1.         1.0007935  0.9981079  ... 1.0006661  0.99931324 0.9998458 ]\n",
      "Layer 30, Head 30: [1.        1.0018921 1.0001221 ... 1.0017899 0.9982495 1.0016646]\n",
      "Layer 30, Head 31: [1.         0.99942017 1.000824   ... 1.000774   0.9984893  0.9992392 ]\n",
      "Layer 31, Head 0: [1.         1.0009766  0.99938965 ... 0.99990845 0.9998604  0.99925447]\n",
      "Layer 31, Head 1: [1.        1.0015869 0.9986572 ... 0.999117  1.0011883 1.0013132]\n",
      "Layer 31, Head 2: [1.        0.9980469 0.998291  ... 1.0003902 1.0002584 0.9992982]\n",
      "Layer 31, Head 3: [1.        1.0019531 0.9975586 ... 1.0017481 1.000127  0.9999155]\n",
      "Layer 31, Head 4: [1.         1.         0.9991455  ... 0.99814475 1.0018348  1.0012947 ]\n",
      "Layer 31, Head 5: [1.         0.9980469  1.0012207  ... 1.0015821  0.99973255 0.9995762 ]\n",
      "Layer 31, Head 6: [1.         0.99902344 0.9998779  ... 0.9991064  1.0003587  0.99938345]\n",
      "Layer 31, Head 7: [1.        1.001709  1.0006104 ... 1.0000297 0.9990552 1.0006105]\n",
      "Layer 31, Head 8: [1.         1.         1.0009766  ... 1.0019071  0.9988269  0.99960256]\n",
      "Layer 31, Head 9: [1.         1.0019531  0.99853516 ... 1.0015482  0.99954724 1.0016476 ]\n",
      "Layer 31, Head 10: [1.         1.         0.99942017 ... 1.0009537  0.9995508  1.0003903 ]\n",
      "Layer 31, Head 11: [1.         1.0019531  0.99913025 ... 0.99917877 0.99913484 1.001049  ]\n",
      "Layer 31, Head 12: [1.         0.9995117  0.99884033 ... 1.0009013  1.00093    1.0007762 ]\n",
      "Layer 31, Head 13: [1.         1.         1.0012207  ... 0.99959123 0.9995074  1.0017247 ]\n",
      "Layer 31, Head 14: [1.         1.0000186  1.0014114  ... 0.99869627 0.99828345 0.99890286]\n",
      "Layer 31, Head 15: [1.        1.0019531 1.        ... 0.9987511 1.0020137 1.0009329]\n",
      "Layer 31, Head 16: [1.         0.998291   0.99868774 ... 1.0012941  1.001153   0.9987993 ]\n",
      "Layer 31, Head 17: [1.        0.9984131 1.0000305 ... 1.0015578 1.0013024 0.9982124]\n",
      "Layer 31, Head 18: [1.         1.0019531  0.99938965 ... 0.9996275  0.9986614  1.0016379 ]\n",
      "Layer 31, Head 19: [1.         0.99890137 0.99816895 ... 0.99901867 1.0017984  1.0004044 ]\n",
      "Layer 31, Head 20: [1.         1.0008545  0.9991455  ... 1.000101   1.00097    0.99897766]\n",
      "Layer 31, Head 21: [1.        0.9992676 1.0004883 ... 0.9994053 0.9987641 0.9989519]\n",
      "Layer 31, Head 22: [1.        1.0005493 1.0002441 ... 1.0016241 0.9985988 1.0004377]\n",
      "Layer 31, Head 23: [1.         1.0013428  0.99938965 ... 1.0002888  0.99950874 0.9985814 ]\n",
      "Layer 31, Head 24: [1.        1.        1.0010986 ... 0.9997921 0.9997785 1.0006453]\n",
      "Layer 31, Head 25: [1.        0.9980469 1.0012817 ... 1.0012124 0.9999945 1.0007346]\n",
      "Layer 31, Head 26: [1.         1.         0.99890137 ... 1.0004117  1.0014141  1.0004305 ]\n",
      "Layer 31, Head 27: [1.        1.0009766 0.9995117 ... 0.9993439 1.0007379 1.0007241]\n",
      "Layer 31, Head 28: [1.         1.001709   0.99902344 ... 1.001674   0.999009   0.99996597]\n",
      "Layer 31, Head 29: [1.        0.9995117 1.0009155 ... 1.000497  0.9997268 1.0003562]\n",
      "Layer 31, Head 30: [1.         0.9980469  1.00177    ... 0.9988325  0.9993882  0.99975157]\n",
      "Layer 31, Head 31: [1.         0.99938965 0.9994812  ... 1.0000447  1.0001214  0.9996767 ]\n"
     ]
    }
   ],
   "source": [
    "# check real attention scores\n",
    "for layer in range(32):\n",
    "    layer_attention = attention_weights[layer]\n",
    "    # Squeeze the batch dimension\n",
    "    attention_pattern = layer_attention.squeeze(0)\n",
    "    for head in range(32):\n",
    "        # Get the specific head's attention matrix and detach from the graph\n",
    "        head_attention = head_attention = attention_pattern[head].detach().to(torch.float32).cpu().numpy()\n",
    "        sum_attention = head_attention.sum(axis=(1))\n",
    "        print(f\"Layer {layer}, Head {head}: {sum_attention}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e889519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape for each layer:\n",
      "Layer 0: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 1: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 2: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 3: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 4: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 5: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 6: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 7: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 8: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 9: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 10: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 11: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 12: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 13: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 14: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 15: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 16: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 17: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 18: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 19: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 20: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 21: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 22: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 23: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 24: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 25: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 26: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 27: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 28: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 29: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 30: torch.Size([1, 32, 3325, 3325])\n",
      "Layer 31: torch.Size([1, 32, 3325, 3325])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = outputs.attentions\n",
    "\n",
    "# Print the shape of the attention weights for each layer\n",
    "print(\"Attention weights shape for each layer:\")\n",
    "for i, layer_attentions in enumerate(attention_weights):\n",
    "    print(f\"Layer {i}: {layer_attentions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d72054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af367695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertical_scores(\n",
    "    avg_mat: np.ndarray,\n",
    "    proximity_ignore: int = 20,\n",
    "    control_depth: bool = True,\n",
    "    score_type: str = \"mean\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate vertical attention scores from an averaged attention matrix.\n",
    "    \"\"\"\n",
    "    n = avg_mat.shape[0]\n",
    "    trius = np.triu_indices_from(avg_mat, k=1)\n",
    "    avg_mat = avg_mat.copy()\n",
    "    avg_mat[trius] = np.nan\n",
    "    trils = np.triu_indices_from(avg_mat, k=-proximity_ignore + 1)\n",
    "    avg_mat[trils] = np.nan\n",
    "\n",
    "    if control_depth:\n",
    "        per_row = np.sum(~np.isnan(avg_mat), axis=1)\n",
    "        avg_mat = stats.rankdata(avg_mat, axis=1, nan_policy=\"omit\") / per_row[:, None]\n",
    "\n",
    "    n = avg_mat.shape[-1]\n",
    "    vert_scores = []\n",
    "    for i in range(n):\n",
    "        vert_lines = avg_mat[i + proximity_ignore :, i]\n",
    "        if score_type == \"mean\":\n",
    "            vert_score = np.nanmean(vert_lines)\n",
    "        elif score_type == \"median\":\n",
    "            vert_score = np.nanmedian(vert_lines)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown score_type: {score_type}\")\n",
    "        vert_scores.append(vert_score)\n",
    "    return np.array(vert_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "621cc81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3554811/1493485245.py:26: RuntimeWarning: Mean of empty slice\n",
      "  vert_score = np.nanmean(vert_lines)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3325)\n"
     ]
    }
   ],
   "source": [
    "all_layer_head_scores = []\n",
    "for layer, layer_attn in enumerate(attention_weights):\n",
    "    layer_scores = []\n",
    "    for head in range(layer_attn.shape[1]):\n",
    "        avg_mat = layer_attn[0, head].detach().to(torch.float32).cpu().numpy()\n",
    "        vert_scores = get_vertical_scores(avg_mat, proximity_ignore=4, control_depth=False, score_type=\"mean\")\n",
    "        layer_scores.append(vert_scores)\n",
    "    all_layer_head_scores.append(layer_scores)\n",
    "all_layer_head_scores = np.array(all_layer_head_scores)  # shape: [num_layers, num_heads, seq_len - proximity_ignore]\n",
    "\n",
    "print(all_layer_head_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c9a281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis shape: (32, 32)\n",
      "[[ 258.62893536  604.88989173 2489.78832809 ... 3306.09810706\n",
      "  3306.99707812 3310.68054525]\n",
      " [3269.12061005 3315.70498374 3315.27292747 ... 3227.84871044\n",
      "  3311.33377638 3313.1016395 ]\n",
      " [3312.13211589 3314.25376148 3304.62134831 ... 3268.93179474\n",
      "  3314.52353338 3312.96820405]\n",
      " ...\n",
      " [3313.91771766 3309.73378565 3315.41659647 ... 3035.77985373\n",
      "  3276.98459069 3311.7976909 ]\n",
      " [2626.25456178 3275.38788134 3287.02868026 ... 3303.01060608\n",
      "  3292.90826847 3120.03892345]\n",
      " [3287.61044035 3287.71816415 3264.56495553 ... 3203.21100153\n",
      "  3241.62140774 2743.03808727]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "def get_3d_ar_kurtosis(layer_head_vert_scores: np.ndarray) -> np.ndarray:\n",
    "    layer_head_kurts = stats.kurtosis(\n",
    "        layer_head_vert_scores, axis=2, fisher=True, bias=True, nan_policy=\"omit\"\n",
    "    )  # NaNs from the proximity ignorance\n",
    "    return layer_head_kurts\n",
    "\n",
    "# Compute kurtosis for all [layer][head] using vertical scores\n",
    "layer_head_kurtosis = get_3d_ar_kurtosis(all_layer_head_scores)\n",
    "print('Kurtosis shape:', layer_head_kurtosis.shape)\n",
    "print(layer_head_kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76df7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ea5ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 kurtosis scores with (layer, head):\n",
      "Layer 4, Head 12: Kurtosis = 3315.9965\n",
      "Layer 1, Head 20: Kurtosis = 3315.9964\n",
      "Layer 3, Head 18: Kurtosis = 3315.9962\n",
      "Layer 25, Head 20: Kurtosis = 3315.9770\n",
      "Layer 3, Head 17: Kurtosis = 3315.9696\n",
      "Layer 3, Head 4: Kurtosis = 3315.9577\n",
      "Layer 29, Head 24: Kurtosis = 3315.9563\n",
      "Layer 25, Head 22: Kurtosis = 3315.9508\n",
      "Layer 1, Head 26: Kurtosis = 3315.9362\n",
      "Layer 1, Head 13: Kurtosis = 3315.9168\n",
      "Layer 1, Head 18: Kurtosis = 3315.9025\n",
      "Layer 29, Head 27: Kurtosis = 3315.9022\n",
      "Layer 29, Head 25: Kurtosis = 3315.8783\n",
      "Layer 23, Head 29: Kurtosis = 3315.8715\n",
      "Layer 29, Head 26: Kurtosis = 3315.8692\n",
      "Layer 28, Head 5: Kurtosis = 3315.8079\n",
      "Layer 28, Head 22: Kurtosis = 3315.8039\n",
      "Layer 22, Head 20: Kurtosis = 3315.7933\n",
      "Layer 24, Head 9: Kurtosis = 3315.7827\n",
      "Layer 7, Head 5: Kurtosis = 3315.7681\n"
     ]
    }
   ],
   "source": [
    "# Find the top 20 kurtosis scores and their (layer, head) indices\n",
    "import numpy as np\n",
    "num_layers, num_heads = layer_head_kurtosis.shape\n",
    "flat_kurtosis = layer_head_kurtosis.flatten()\n",
    "top_indices = np.argpartition(flat_kurtosis, -20)[-20:]\n",
    "top_sorted_indices = top_indices[np.argsort(flat_kurtosis[top_indices])[::-1]]\n",
    "print(\"Top 20 kurtosis scores with (layer, head):\")\n",
    "for idx in top_sorted_indices:\n",
    "    layer = idx // num_heads\n",
    "    head = idx % num_heads\n",
    "    score = layer_head_kurtosis[layer, head]\n",
    "    print(f\"Layer {layer}, Head {head}: Kurtosis = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3404dcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(layer_head_kurtosis, cmap='viridis', annot=False, fmt=\".2f\")\n",
    "plt.title('Kurtosis of Vertical Attention Scores by Layer and Head', fontsize=16)\n",
    "plt.xlabel('Heads')\n",
    "plt.ylabel('Layers')\n",
    "plt.xticks(np.arange(0, 32, 2), np.arange(0, 32, 2))\n",
    "plt.yticks(np.arange(0, 32, 2), np.arange(0, 32, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12bced1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 heads by kurtosis (layer, head):\n",
      " 1: Layer 4, Head 12, Kurtosis: 3315.997\n",
      " 2: Layer 1, Head 20, Kurtosis: 3315.996\n",
      " 3: Layer 3, Head 18, Kurtosis: 3315.996\n",
      " 4: Layer 25, Head 20, Kurtosis: 3315.977\n",
      " 5: Layer 3, Head 17, Kurtosis: 3315.970\n",
      " 6: Layer 3, Head 4, Kurtosis: 3315.958\n",
      " 7: Layer 29, Head 24, Kurtosis: 3315.956\n",
      " 8: Layer 25, Head 22, Kurtosis: 3315.951\n",
      " 9: Layer 1, Head 26, Kurtosis: 3315.936\n",
      "10: Layer 1, Head 13, Kurtosis: 3315.917\n",
      "11: Layer 1, Head 18, Kurtosis: 3315.903\n",
      "12: Layer 29, Head 27, Kurtosis: 3315.902\n",
      "13: Layer 29, Head 25, Kurtosis: 3315.878\n",
      "14: Layer 23, Head 29, Kurtosis: 3315.871\n",
      "15: Layer 29, Head 26, Kurtosis: 3315.869\n",
      "16: Layer 28, Head 5, Kurtosis: 3315.808\n",
      "17: Layer 28, Head 22, Kurtosis: 3315.804\n",
      "18: Layer 22, Head 20, Kurtosis: 3315.793\n",
      "19: Layer 24, Head 9, Kurtosis: 3315.783\n",
      "20: Layer 7, Head 5, Kurtosis: 3315.768\n"
     ]
    }
   ],
   "source": [
    "num_top = 20  # Number of heads to select\n",
    "# Flatten and get indices of top 20 kurtosis values\n",
    "flat_indices = np.argsort(layer_head_kurtosis.flatten())[::-1][:num_top]\n",
    "layer_indices, head_indices = np.unravel_index(flat_indices, layer_head_kurtosis.shape)\n",
    "top_20_heads = list(zip(layer_indices, head_indices))\n",
    "\n",
    "print(\"Top 20 heads by kurtosis (layer, head):\")\n",
    "for i, (layer, head) in enumerate(top_20_heads):\n",
    "    print(f\"{i+1:2d}: Layer {layer}, Head {head}, Kurtosis: {layer_head_kurtosis[layer, head]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "flat_kurtosis = layer_head_kurtosis.flatten()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(flat_kurtosis, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Kurtosis Scores for All (Layer, Head) Pairs')\n",
    "plt.xlabel('Kurtosis')\n",
    "plt.ylabel('Count')\n",
    "# Plot percentiles\n",
    "for p in [50, 75, 90, 95, 99]:\n",
    "    perc = np.percentile(flat_kurtosis, p)\n",
    "    plt.axvline(perc, color='red', linestyle='--', label=f'{p}th percentile: {perc:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b70d5c5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unsupported ScalarType BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m attention_pattern = layer_attention.squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Get the specific head's attention matrix and detach from the graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m head_attention = \u001b[43mattention_pattern\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhead_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Create the heatmap visualization\u001b[39;00m\n\u001b[32m     14\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m8\u001b[39m))\n",
      "\u001b[31mTypeError\u001b[39m: Got unsupported ScalarType BFloat16"
     ]
    }
   ],
   "source": [
    "# Define the layer and head you want to visualize\n",
    "layer_index = 4\n",
    "head_index = 12\n",
    "\n",
    "# Extract the attention pattern for the specified layer and head\n",
    "if layer_index < len(attention_weights):\n",
    "    layer_attention = attention_weights[layer_index]\n",
    "    # Squeeze the batch dimension\n",
    "    attention_pattern = layer_attention.squeeze(0)\n",
    "    # Get the specific head's attention matrix and detach from the graph\n",
    "    head_attention = attention_pattern[head_index].detach().numpy()\n",
    "\n",
    "    # Create the heatmap visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # sns.heatmap(head_attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
    "    sns.heatmap(head_attention, cmap='viridis')\n",
    "    plt.title(f'Attention Head {head_index} in Layer {layer_index}')\n",
    "    plt.xlabel('Keys')\n",
    "    plt.ylabel('Queries')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Invalid layer index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4f9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
