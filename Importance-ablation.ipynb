{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02003ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reciever_heads = [(4, 12),\n",
    " (1, 20),\n",
    " (1, 18),\n",
    " (3, 18),\n",
    " (4, 14),\n",
    " (25, 20),\n",
    " (25, 22),\n",
    " (3, 17),\n",
    " (29, 24),\n",
    " (1, 13),\n",
    " (29, 25),\n",
    " (29, 27),\n",
    " (29, 26),\n",
    " (24, 9),\n",
    " (1, 26),\n",
    " (29, 15),\n",
    " (24, 11),\n",
    " (3, 4),\n",
    " (7, 5),\n",
    " (1, 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b4080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd61c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablated_model = copy.deepcopy(model)\n",
    "layer_index_to_ablate = 31\n",
    "head_index_to_ablate = 14\n",
    "\n",
    "def ablate_attention_hook(module, input, output):\n",
    "    \"\"\"\n",
    "    Forward hook to ablate a specific attention head's output.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): The module to which the hook is attached.\n",
    "        input (tuple): The input to the module.\n",
    "        output (torch.Tensor): The output of the module.\n",
    "    \"\"\"\n",
    "    # The output of the attention layer is typically a tuple. The first element is the\n",
    "    # tensor containing the combined head outputs.\n",
    "    attention_output = output[0]\n",
    "\n",
    "    # Get the dimensions of the attention output tensor\n",
    "    batch_size, sequence_length, hidden_dim = attention_output.shape\n",
    "\n",
    "    # Find the size of each head's output dimension\n",
    "    # It's hidden_dim / num_heads. This needs to be calculated from the model config.\n",
    "    num_heads = module.num_heads\n",
    "    head_dim = hidden_dim // num_heads\n",
    "\n",
    "    # Reshape the output to separate heads\n",
    "    # The shape becomes (batch_size, sequence_length, num_heads, head_dim)\n",
    "    reshaped_output = attention_output.view(batch_size, sequence_length, num_heads, head_dim)\n",
    "\n",
    "    # Abiate the specified head by setting its values to zero\n",
    "    # Use torch.zeros_like to maintain the correct data type and device\n",
    "    reshaped_output[:, :, head_index_to_ablate, :] = torch.zeros_like(\n",
    "        reshaped_output[:, :, head_index_to_ablate, :]\n",
    "    )\n",
    "\n",
    "    # Reshape the tensor back to its original shape\n",
    "    # This prepares it for the next module in the network\n",
    "    modified_output = reshaped_output.view(batch_size, sequence_length, hidden_dim)\n",
    "\n",
    "    # Return the modified output. It must be returned as a tuple to match the original output format.\n",
    "    return (modified_output,) + output[1:]\n",
    "\n",
    "# --- Register the hook on the specific attention layer ---\n",
    "# We need to find the correct module. In most transformer models, the attention modules are\n",
    "# located within the decoder layers.\n",
    "# The structure is often model.base_model.layers[layer_index].self_attn\n",
    "# Let's verify the structure for the DeepSeek-R1-Distill-Llama-8B model\n",
    "attention_layer = ablated_model.model.layers[layer_index_to_ablate].self_attn\n",
    "\n",
    "# Register the hook. The hook will now be triggered on every forward pass of this module.\n",
    "attention_layer.register_forward_hook(ablate_attention_hook)\n",
    "\n",
    "# --- Tokenize the input text ---\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# --- Generate text with the original and ablated models ---\n",
    "# The original model will generate text normally\n",
    "original_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "original_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "\n",
    "# The ablated model's specified head output will be zeroed out\n",
    "ablated_output = ablated_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "ablated_text = tokenizer.decode(ablated_output[0], skip_special_tokens=True)\n",
    "\n",
    "# --- Print the results for comparison ---\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original Text:\")\n",
    "print(original_text)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Ablated Text (Layer {layer_index_to_ablate}, Head {head_index_to_ablate}):\")\n",
    "print(ablated_text)\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6bf741",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6d815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
